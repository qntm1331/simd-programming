diff --git a/.gitignore b/.gitignore
index 93a2665..08fe74c 100644
--- a/.gitignore
+++ b/.gitignore
@@ -40,3 +40,432 @@ build/
 
 # debug information files
 *.dwo
+
+## Ignore Visual Studio temporary files, build results, and
+## files generated by popular Visual Studio add-ons.
+##
+## Get latest from https://github.com/github/gitignore/blob/main/VisualStudio.gitignore
+
+# User-specific files
+*.rsuser
+*.suo
+*.user
+*.userosscache
+*.sln.docstates
+*.env
+
+# User-specific files (MonoDevelop/Xamarin Studio)
+*.userprefs
+
+# Mono auto generated files
+mono_crash.*
+
+# Build results
+[Dd]ebug/
+[Dd]ebugPublic/
+[Rr]elease/
+[Rr]eleases/
+
+[Dd]ebug/x64/
+[Dd]ebugPublic/x64/
+[Rr]elease/x64/
+[Rr]eleases/x64/
+bin/x64/
+obj/x64/
+
+[Dd]ebug/x86/
+[Dd]ebugPublic/x86/
+[Rr]elease/x86/
+[Rr]eleases/x86/
+bin/x86/
+obj/x86/
+
+[Ww][Ii][Nn]32/
+[Aa][Rr][Mm]/
+[Aa][Rr][Mm]64/
+[Aa][Rr][Mm]64[Ee][Cc]/
+bld/
+[Oo]bj/
+[Oo]ut/
+[Ll]og/
+[Ll]ogs/
+
+# Build results on 'Bin' directories
+**/[Bb]in/*
+# Uncomment if you have tasks that rely on *.refresh files to move binaries
+# (https://github.com/github/gitignore/pull/3736)
+#!**/[Bb]in/*.refresh
+
+# Visual Studio 2015/2017 cache/options directory
+.vs/
+# Uncomment if you have tasks that create the project's static files in wwwroot
+#wwwroot/
+
+# Visual Studio 2017 auto generated files
+Generated\ Files/
+
+# MSTest test Results
+[Tt]est[Rr]esult*/
+[Bb]uild[Ll]og.*
+*.trx
+
+# NUnit
+*.VisualState.xml
+TestResult.xml
+nunit-*.xml
+
+# Approval Tests result files
+*.received.*
+
+# Build Results of an ATL Project
+[Dd]ebugPS/
+[Rr]eleasePS/
+dlldata.c
+
+# Benchmark Results
+BenchmarkDotNet.Artifacts/
+
+# .NET Core
+project.lock.json
+project.fragment.lock.json
+artifacts/
+
+# ASP.NET Scaffolding
+ScaffoldingReadMe.txt
+
+# StyleCop
+StyleCopReport.xml
+
+# Files built by Visual Studio
+*_i.c
+*_p.c
+*_h.h
+*.ilk
+*.meta
+*.obj
+*.idb
+*.iobj
+*.pch
+*.pdb
+*.ipdb
+*.pgc
+*.pgd
+*.rsp
+# but not Directory.Build.rsp, as it configures directory-level build defaults
+!Directory.Build.rsp
+*.sbr
+*.tlb
+*.tli
+*.tlh
+*.tmp
+*.tmp_proj
+*_wpftmp.csproj
+*.log
+*.tlog
+*.vspscc
+*.vssscc
+.builds
+*.pidb
+*.svclog
+*.scc
+
+# Chutzpah Test files
+_Chutzpah*
+
+# Visual C++ cache files
+ipch/
+*.aps
+*.ncb
+*.opendb
+*.opensdf
+*.sdf
+*.cachefile
+*.VC.db
+*.VC.VC.opendb
+
+# Visual Studio profiler
+*.psess
+*.vsp
+*.vspx
+*.sap
+
+# Visual Studio Trace Files
+*.e2e
+
+# TFS 2012 Local Workspace
+$tf/
+
+# Guidance Automation Toolkit
+*.gpState
+
+# ReSharper is a .NET coding add-in
+_ReSharper*/
+*.[Rr]e[Ss]harper
+*.DotSettings.user
+
+# TeamCity is a build add-in
+_TeamCity*
+
+# DotCover is a Code Coverage Tool
+*.dotCover
+
+# AxoCover is a Code Coverage Tool
+.axoCover/*
+!.axoCover/settings.json
+
+# Coverlet is a free, cross platform Code Coverage Tool
+coverage*.json
+coverage*.xml
+coverage*.info
+
+# Visual Studio code coverage results
+*.coverage
+*.coveragexml
+
+# NCrunch
+_NCrunch_*
+.NCrunch_*
+.*crunch*.local.xml
+nCrunchTemp_*
+
+# MightyMoose
+*.mm.*
+AutoTest.Net/
+
+# Web workbench (sass)
+.sass-cache/
+
+# Installshield output folder
+[Ee]xpress/
+
+# DocProject is a documentation generator add-in
+DocProject/buildhelp/
+DocProject/Help/*.HxT
+DocProject/Help/*.HxC
+DocProject/Help/*.hhc
+DocProject/Help/*.hhk
+DocProject/Help/*.hhp
+DocProject/Help/Html2
+DocProject/Help/html
+
+# Click-Once directory
+publish/
+
+# Publish Web Output
+*.[Pp]ublish.xml
+*.azurePubxml
+# Note: Comment the next line if you want to checkin your web deploy settings,
+# but database connection strings (with potential passwords) will be unencrypted
+*.pubxml
+*.publishproj
+
+# Microsoft Azure Web App publish settings. Comment the next line if you want to
+# checkin your Azure Web App publish settings, but sensitive information contained
+# in these scripts will be unencrypted
+PublishScripts/
+
+# NuGet Packages
+*.nupkg
+# NuGet Symbol Packages
+*.snupkg
+# The packages folder can be ignored because of Package Restore
+**/[Pp]ackages/*
+# except build/, which is used as an MSBuild target.
+!**/[Pp]ackages/build/
+# Uncomment if necessary however generally it will be regenerated when needed
+#!**/[Pp]ackages/repositories.config
+# NuGet v3's project.json files produces more ignorable files
+*.nuget.props
+*.nuget.targets
+
+# Microsoft Azure Build Output
+csx/
+*.build.csdef
+
+# Microsoft Azure Emulator
+ecf/
+rcf/
+
+# Windows Store app package directories and files
+AppPackages/
+BundleArtifacts/
+Package.StoreAssociation.xml
+_pkginfo.txt
+*.appx
+*.appxbundle
+*.appxupload
+
+# Visual Studio cache files
+# files ending in .cache can be ignored
+*.[Cc]ache
+# but keep track of directories ending in .cache
+!?*.[Cc]ache/
+
+# Others
+ClientBin/
+~$*
+*~
+*.dbmdl
+*.dbproj.schemaview
+*.jfm
+*.pfx
+*.publishsettings
+orleans.codegen.cs
+
+# Including strong name files can present a security risk
+# (https://github.com/github/gitignore/pull/2483#issue-259490424)
+#*.snk
+
+# Since there are multiple workflows, uncomment next line to ignore bower_components
+# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
+#bower_components/
+
+# RIA/Silverlight projects
+Generated_Code/
+
+# Backup & report files from converting an old project file
+# to a newer Visual Studio version. Backup files are not needed,
+# because we have git ;-)
+_UpgradeReport_Files/
+Backup*/
+UpgradeLog*.XML
+UpgradeLog*.htm
+ServiceFabricBackup/
+*.rptproj.bak
+
+# SQL Server files
+*.mdf
+*.ldf
+*.ndf
+
+# Business Intelligence projects
+*.rdl.data
+*.bim.layout
+*.bim_*.settings
+*.rptproj.rsuser
+*- [Bb]ackup.rdl
+*- [Bb]ackup ([0-9]).rdl
+*- [Bb]ackup ([0-9][0-9]).rdl
+
+# Microsoft Fakes
+FakesAssemblies/
+
+# GhostDoc plugin setting file
+*.GhostDoc.xml
+
+# Node.js Tools for Visual Studio
+.ntvs_analysis.dat
+node_modules/
+
+# Visual Studio 6 build log
+*.plg
+
+# Visual Studio 6 workspace options file
+*.opt
+
+# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
+*.vbw
+
+# Visual Studio 6 workspace and project file (working project files containing files to include in project)
+*.dsw
+*.dsp
+
+# Visual Studio 6 technical files
+*.ncb
+*.aps
+
+# Visual Studio LightSwitch build output
+**/*.HTMLClient/GeneratedArtifacts
+**/*.DesktopClient/GeneratedArtifacts
+**/*.DesktopClient/ModelManifest.xml
+**/*.Server/GeneratedArtifacts
+**/*.Server/ModelManifest.xml
+_Pvt_Extensions
+
+# Paket dependency manager
+**/.paket/paket.exe
+paket-files/
+
+# FAKE - F# Make
+**/.fake/
+
+# CodeRush personal settings
+**/.cr/personal
+
+# Python Tools for Visual Studio (PTVS)
+**/__pycache__/
+*.pyc
+
+# Cake - Uncomment if you are using it
+#tools/**
+#!tools/packages.config
+
+# Tabs Studio
+*.tss
+
+# Telerik's JustMock configuration file
+*.jmconfig
+
+# BizTalk build output
+*.btp.cs
+*.btm.cs
+*.odx.cs
+*.xsd.cs
+
+# OpenCover UI analysis results
+OpenCover/
+
+# Azure Stream Analytics local run output
+ASALocalRun/
+
+# MSBuild Binary and Structured Log
+*.binlog
+MSBuild_Logs/
+
+# AWS SAM Build and Temporary Artifacts folder
+.aws-sam
+
+# NVidia Nsight GPU debugger configuration file
+*.nvuser
+
+# MFractors (Xamarin productivity tool) working folder
+**/.mfractor/
+
+# Local History for Visual Studio
+**/.localhistory/
+
+# Visual Studio History (VSHistory) files
+.vshistory/
+
+# BeatPulse healthcheck temp database
+healthchecksdb
+
+# Backup folder for Package Reference Convert tool in Visual Studio 2017
+MigrationBackup/
+
+# Ionide (cross platform F# VS Code tools) working folder
+**/.ionide/
+
+# Fody - auto-generated XML schema
+FodyWeavers.xsd
+
+# VS Code files for those working on multiple tools
+.vscode/*
+!.vscode/settings.json
+!.vscode/tasks.json
+!.vscode/launch.json
+!.vscode/extensions.json
+!.vscode/*.code-snippets
+
+# Local History for Visual Studio Code
+.history/
+
+# Built Visual Studio Code Extensions
+*.vsix
+
+# Windows Installer files from build outputs
+*.cab
+*.msi
+*.msix
+*.msm
+*.msp
diff --git a/01/binding_with_unions/Makefile b/01/binding_with_unions/Makefile
deleted file mode 100644
index ce35a5f..0000000
--- a/01/binding_with_unions/Makefile
+++ /dev/null
@@ -1,23 +0,0 @@
-CXX=g++
-CXXFLAGS=-mavx2 -masm=att -std=c++11
-TARGET=simd_program
-BUILDDIR=build
-ASMFILE=$(BUILDDIR)/main.s
-SRCFILE=main.cpp
-OBJFILE=$(BUILDDIR)/main.o
-EXECUTABLE=$(BUILDDIR)/$(TARGET)
-
-all: $(EXECUTABLE)
-
-$(EXECUTABLE): $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) $(SRCFILE) -o $(EXECUTABLE)
-
-asm: $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) -S $(SRCFILE) -o $(ASMFILE)
-
-$(BUILDDIR):
-	mkdir -p $(BUILDDIR)
-
-clean:
-	rm -rf $(BUILDDIR) $(TARGET)
-
diff --git a/01/binding_with_unions/README.md b/01/binding_with_unions/README.md
deleted file mode 100644
index 00e556f..0000000
--- a/01/binding_with_unions/README.md
+++ /dev/null
@@ -1,322 +0,0 @@
-
-##  SIMD 벡터의 데이터에 접근하는 4가지 방법
-
-SIMD 레지스터(__m256)는 특수한 타입이라 일반 배열처럼 vec[0], vec[1] 같은 방식으로 접근할 수 없다.
-데이터를 읽거나 수정하려면 특별한 방법이 필요하다.
-
-### 1. 포인터 변환 (reinterpret_cast)
-
-```cpp
-__m256 simd_vec = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);
-float* float_ptr = reinterpret_cast<float*>(&simd_vec);
-
-float_ptr[0] = 100.0f;  // 첫 번째 요소 수정
-```
-
-- __m256 변수의 주소를 float*로 재해석
-- 메모리는 같은데 타입만 바꿔서 배열처럼 접근
-- 장점: 간단하고 직관적
-- 단점: 타입 안전성 없음, 정렬 문제 가능
-
-### 2. Union 사용
-
-한 번의 메모리 할당으로 SIMD 타입과 배열 타입 모두로 사용 가능.
-메모리 복사 없이 단지 "해석 방법"만 바꾸는 것
-```cpp
-union FloatSIMD {
-    __m256 v;      // SIMD 벡터
-    float a[8];    // 일반 배열
-};
-
-FloatSIMD data;
-data.v = _mm256_set_ps(...);  // SIMD로 초기화
-data.a[0] = 100.0f;           // 배열로 접근
-
-
-// SIMD 연산은 v로
-data.v = _mm256_add_ps(x, y);
-
-// 개별 접근은 a로
-if (data.a[3] > 100.0f) {
-    data.a[3] = 100.0f;
-}
-
-// 다시 SIMD 연산
-result = _mm256_mul_ps(data.v, scale);
-```
-
-- Union은 같은 메모리를 여러 타입으로 공유
-- v와 a가 같은 32바이트 메모리를 가리킴
-
-메모리 레이아웃
-
-```cpp
-+---+---+---+---+---+---+---+---+
-| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |  ← a[8]
-+---+---+---+---+---+---+---+---+
-|          __m256 v              |
-```
-
-- 장점: 더 안전하고 명시적
-- 단점: Union 자체의 타입 안전성 제한
-
-### 3. Store/Load 함수 (권장)
-
-```cpp
-__m256 vec = _mm256_set_ps(8, 7, 6, 5, 4, 3, 2, 1);
-
-// SIMD → 배열
-float* array = aligned_alloc<float>(8);
-_mm256_store_ps(array, vec);
-
-// 배열 수정
-array[3] = 30.0f;
-
-// 배열 → SIMD
-vec = _mm256_load_ps(array);
-```
-
-작동 원리
-
-- _mm256_store_ps: SIMD 레지스터 → 메모리
-- _mm256_load_ps: 메모리 → SIMD 레지스터
-- 명시적인 데이터 전송
-
-aligned_alloc이 필요한 이유
-
-- _mm256_store_ps는 32바이트 정렬 필요
-- 정렬되지 않으면 크래시 발생
-- _mm256_storeu_ps는 정렬 불필요
-
-- 장점: 명확하고 안전, 최적화 가능
-- 단점: 코드가 약간 길어짐
-
-### 4. Extract/Insert (개별 요소)
-
-```cpp
-__m256i vec = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1);
-
-// 추출: 256비트 → 128비트 → 32비트
-__m128i low = _mm256_extracti128_si256(vec, 0);   // 하위 4개
-int val = _mm_extract_epi32(low, 0);              // 첫 번째 요소
-
-// 삽입: 32비트 → 128비트 → 256비트
-__m128i new_low = _mm_insert_epi32(low, 100, 1);  // 요소 1 변경
-vec = _mm256_setr_m128i(new_low, high);           // 재조립
-```
-
-- AVX2는 256비트에서 직접 32비트 요소 접근 불가
-- 128비트 레인으로 나눠서 접근해야 함
-
-레인 구조
-
-```bash
-__m256i (256비트)
-├─ low  (128비트): [0][1][2][3]
-└─ high (128비트): [4][5][6][7]
-```
-
-- 장점: 1-2개 요소만 수정할 때 효율적
-- 단점: 여러 요소 접근 시 복잡하고 느림
-
----
-
-### 사용 시나리오
-
-전체 벡터 처리 → Store/Load
-```cpp
-__m256 result = _mm256_add_ps(a, b);
-float output[8];
-_mm256_storeu_ps(output, result);
-```
-
-디버깅/출력 → Union
-
-```cpp
-float8 debug;
-debug.v = some_simd_vector;
-for (int i = 0; i < 8; i++) {
-    printf("%f ", debug.a[i]);
-}
-```
-
-특정 요소만 수정 → Extract/Insert
-
-```cpp
-// 벡터의 첫 번째 요소만 0으로 변경
-__m128i low = _mm256_extracti128_si256(vec, 0);
-low = _mm_insert_epi32(low, 0, 0);
-vec = _mm256_inserti128_si256(vec, low, 0);
-```
-
-빠른 프로토타입 → reinterpret_cast
-
-```cpp
-float* ptr = reinterpret_cast<float*>(&vec);
-// 주의: 프로덕션 코드에는 권장 안 함
-```
-
-### Store/Load 과정
-
-CPU는 데이터를 효율적으로 읽기 위해 특정 주소 경계에서 읽음
-
-Store/Load는 레지스터와 메모리 간의 명시적인 데이터 복사.
-
-Union과 달리 데이터가 실제로 이동하지만, 정렬된 메모리를 사용하면 매우 빠르게 처리.
-
-```cpp
-// 정렬되지 않은 메모리
-float array[8];  // 주소가 0x1003 같은 임의의 위치일 수 있음
-
-// 32바이트 정렬된 메모리
-float* aligned = aligned_alloc<float>(8);  // 주소가 0x1000, 0x1020 등
-```
-
-정렬된 주소
-```
-좋은 주소 (32바이트 정렬):
-0x0000, 0x0020, 0x0040, 0x0060 ...
-↑ 32(0x20)의 배수
-
-나쁜 주소 (정렬 안 됨):
-0x0003, 0x0017, 0x002B ...
-```
-
-#### Store/Load 과정
-
-- 1. SIMD 레지스터 → 메모리 (Store)
-    ```
-    __m256 vec = _mm256_set_ps(8, 7, 6, 5, 4, 3, 2, 1);
-    float* array = aligned_alloc<float>(8);
-    _mm256_store_ps(array, vec);
-    ```
-    메모리 변화
-    ```
-        [레지스터]                    [메모리]
-    __m256 vec                    float array[8]
-    ┌────────────┐                ┌─────┐
-    │ 1 2 3 4    │  store_ps      │ 1.0 │ array[0] (주소: 0x1000)
-    │ 5 6 7 8    │  ========>     │ 2.0 │ array[1] (주소: 0x1004)
-    └────────────┘                │ 3.0 │ array[2] (주소: 0x1008)
-      (256비트)                   │ 4.0 │ array[3] (주소: 0x100C)
-                                  │ 5.0 │ array[4] (주소: 0x1010)
-                                  │ 6.0 │ array[5] (주소: 0x1014)
-                                  │ 7.0 │ array[6] (주소: 0x1018)
-                                  │ 8.0 │ array[7] (주소: 0x101C)
-                                  └─────┘
-                                   (32바이트)
-    ```
-    CPU가 하는 일
-    - 레지스터의 256비트를 한 번에 메모리에 쓰기
-    - 32바이트 정렬된 주소라서 1번의 메모리 쓰기로 완료
-- 2. 메모리 수정
-    ```
-    array[3] = 30.0f;
-    array[7] = 80.0f;
-    ```
-    메모리 상태
-    ```
-    주소      값
-    0x1000:  1.0   array[0]
-    0x1004:  2.0   array[1]
-    0x1008:  3.0   array[2]
-    0x100C:  30.0  array[3] ← 수정
-    0x1010:  5.0   array[4]
-    0x1014:  6.0   array[5]
-    0x1018:  7.0   array[6]
-    0x101C:  80.0  array[7] ← 수정
-    ```
-- 3. 메모리 → SIMD 레지스터 (Load)
-    ```cpp
-    __m256 modified = _mm256_load_ps(array);
-    ```
-    메모리에서 레지스터로
-    ```
-    [메모리]                      [레지스터]
-    float array[8]                __m256 modified
-    ┌─────┐                      ┌────────────┐
-    │ 1.0 │ 0x1000               │ 1 2 3 30   │
-    │ 2.0 │ 0x1004    load_ps    │ 5 6 7 80   │
-    │ 3.0 │ 0x1008   ========>   └────────────┘
-    │ 30.0│ 0x100C                 (256비트)
-    │ 5.0 │ 0x1010
-    │ 6.0 │ 0x1014
-    │ 7.0 │ 0x1018
-    │ 80.0│ 0x101C
-    └─────┘
-    ```
-    CPU가 하는 일
-    - 메모리의 32바이트를 한 번에 레지스터로 읽기
-    - 정렬되어 있어서 1번의 메모리 읽기로 완료
-
-
-#### 정렬된 vs 정렬 안 된 접근
-
-정렬된 접근 (_mm256_store_ps)
-
-```
-메모리 주소: 0x1000 (32바이트 정렬)
-┌──────────────────────────────────┐
-│  32바이트를 한 번에 읽기/쓰기     │
-└──────────────────────────────────┘
-CPU 버스 폭과 딱 맞음 → 빠름
-```
-
-정렬 안 된 접근 (_mm256_storeu_ps)
-
-```
-메모리 주소: 0x1003 (정렬 안 됨)
-   ┌──────────────────────────────────┐
-   │  경계를 넘어감                   │
-   └──────────────────────────────────┘
-┌─────┐                           ┌─────┐
-│ 1번 │                           │ 2번 │
-└─────┘                           └─────┘
-2번의 메모리 접근 필요 → 느림
-```
-
-
-#### 실제 어셈블리 레벨
-
-```cpp
-_mm256_store_ps(array, vec);
-```
-
-생성되는 어셈블리
-
-```
-vmovaps ymm0, [rdi]    ; 1개 명령어 (aligned)
-```
-
-```cpp
-_mm256_storeu_ps(array, vec);
-```
-
-생성되는 어셈블리
-
-```
-vmovups ymm0, [rdi]    ; 1개 명령어지만 내부적으로 느림 (unaligned)
-```
-
-#### 전체 데이터 흐름
-
-```
-1. SIMD 연산
-   __m256 result = _mm256_add_ps(a, b);
-   [레지스터] 1 2 3 4 5 6 7 8
-
-2. Store (레지스터 → 메모리)
-   _mm256_store_ps(array, result);
-   [메모리] 0x1000: 1 2 3 4 5 6 7 8
-
-3. 일반 코드로 수정
-   array[3] = 100;
-   [메모리] 0x1000: 1 2 3 100 5 6 7 8
-
-4. Load (메모리 → 레지스터)
-   __m256 modified = _mm256_load_ps(array);
-   [레지스터] 1 2 3 100 5 6 7 8
-
-5. 다시 SIMD 연산
-   result = _mm256_mul_ps(modified, scale);
-```
diff --git a/01/binding_with_unions/main.cpp b/01/binding_with_unions/main.cpp
deleted file mode 100644
index a85ccd6..0000000
--- a/01/binding_with_unions/main.cpp
+++ /dev/null
@@ -1,168 +0,0 @@
-#include "../../include/simd_utils.h"
-#include <iostream>
-#include <iomanip>
-
-/**
- * SIMD 데이터 접근 기법들
- *
- * SIMD 벡터의 데이터에 접근하고 조작하는 방법
- * 1. 포인터 변환 사용 (reinterpret_cast)
- * 2. Union을 사용하여 SIMD 타입과 배열 간 alias 생성
- * 3. _mm256_store_*와 _mm256_load_* 함수 사용
- * 4. Extract와 Insert 함수로 개별 요소 접근
- *
- */
-
-int main() {
-    std::cout << "=== SIMD 데이터 접근하기 ===" << std::endl;
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 1. 포인터 변환 (Pointer Conversion)
-    // ========================================================================
-    std::cout << "1. 포인터 변환 (Pointer Conversion)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "reinterpret_cast를 사용하여 SIMD 타입과 배열 간 변환합니다." << std::endl;
-    std::cout << "간단하지만 잠재적으로 안전하지 않은 방법입니다." << std::endl;
-    std::cout << std::endl;
-
-    // 오름차순 값으로 SIMD 벡터 초기화
-    __m256 simd_vec1 = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);
-
-    // 포인터 변환을 사용하여 데이터 접근
-    // __m256의 주소를 float*로 재해석 → 배열처럼 접근 가능
-    float* float_ptr = reinterpret_cast<float*>(&simd_vec1);
-
-    std::cout << "포인터를 통한 SIMD 벡터 값: [";
-    for (int i = 0; i < 7; i++) {
-        std::cout << float_ptr[i] << ", ";
-    }
-    std::cout << float_ptr[7] << "]" << std::endl;
-
-    // 포인터를 통해 데이터 수정
-    std::cout << "포인터를 통해 값 수정 중..." << std::endl;
-    float_ptr[0] = 100.0f;
-    float_ptr[4] = 200.0f;
-
-    print_m256(simd_vec1, "수정된 SIMD 벡터");
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 2. Union 사용
-    // ========================================================================
-    std::cout << "2. Union 사용" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "Union을 사용하여 SIMD 타입과 배열 간 별칭을 생성합니다." << std::endl;
-    std::cout << "포인터 변환보다 깔끔하고 안전한 접근 방식입니다." << std::endl;
-    std::cout << std::endl;
-
-    // float SIMD 벡터용 union 정의
-    // v와 a는 같은 32바이트 메모리를 공유함
-    union FloatSIMD {
-        __m256 v;       // SIMD 벡터로 접근
-        float a[8];     // 배열로 접근
-    };
-
-    // SIMD 벡터로 union 초기화
-    FloatSIMD float_union;
-    float_union.v = _mm256_set_ps(16.0f, 14.0f, 12.0f, 10.0f, 8.0f, 6.0f, 4.0f, 2.0f);
-
-    // 배열을 통해 데이터 접근
-    std::cout << "Union을 통한 SIMD 벡터 값: [";
-    for (int i = 0; i < 7; i++) {
-        std::cout << float_union.a[i] << ", ";
-    }
-    std::cout << float_union.a[7] << "]" << std::endl;
-
-    // 배열을 통해 데이터 수정
-    std::cout << "Union을 통해 값 수정 중..." << std::endl;
-    float_union.a[1] = 42.0f;   // 배열로 수정하면
-    float_union.a[6] = 99.0f;   // SIMD 벡터에도 반영됨 (같은 메모리)
-
-    print_m256(float_union.v, "수정된 SIMD 벡터 (union)");
-
-    float8 float8_union;
-    float8_union.v = _mm256_set1_ps(5.0f);  // 모든 레인을 5.0으로
-    float8_union.a[2] = 10.0f;              // 3번째 index만 10.0으로
-    float8_union.a[5] = 20.0f;              // 6번째 index만 20.0으로
-
-    print_m256(float8_union.v, "simd_utils.h의 float8 union 사용");
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 3. Store와 Load 함수 (권장 방법)
-    // ========================================================================
-    std::cout << "3. Store와 Load 함수" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "_mm256_store_*와 _mm256_load_* 함수로 데이터를 전송합니다." << std::endl;
-    std::cout << "대부분의 상황에서 권장되는 접근 방식입니다." << std::endl;
-    std::cout << std::endl;
-
-    // SIMD 벡터 초기화
-    __m256 simd_vec3 = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);
-
-    // 정렬된 메모리 할당 (32바이트 정렬)
-    // _mm256_store_ps는 정렬된 메모리를 요구함
-    float* aligned_array = aligned_alloc<float>(8);
-
-    // SIMD 벡터를 배열에 저장 (레지스터 → 메모리)
-    _mm256_store_ps(aligned_array, simd_vec3);
-
-    std::cout << "Store를 통한 SIMD 벡터 값: [";
-    for (int i = 0; i < 7; i++) {
-        std::cout << aligned_array[i] << ", ";
-    }
-    std::cout << aligned_array[7] << "]" << std::endl;
-
-    // 배열 수정 (일반 C++ 코드)
-    std::cout << "배열 값 수정 중..." << std::endl;
-    aligned_array[3] = 30.0f;
-    aligned_array[7] = 80.0f;
-
-    // 수정된 배열을 SIMD 벡터로 로드 (메모리 → 레지스터)
-    __m256 modified_vec = _mm256_load_ps(aligned_array);
-
-    print_m256(modified_vec, "수정된 SIMD 벡터 (store/load)");
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 4. Extract와 Insert 요소
-    // ========================================================================
-    std::cout << "4. Extract와 Insert 요소" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "_mm256_extract_*와 _mm256_insert_* 함수로 개별 요소에 접근합니다." << std::endl;
-    std::cout << "몇 개의 요소만 접근할 때 유용합니다." << std::endl;
-    std::cout << std::endl;
-
-    // 정수로 SIMD 벡터 초기화
-    __m256i simd_int_vec = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1);
-
-    // 개별 추출
-    // 주의: AVX2에서는 먼저 128비트 레인을 추출한 후, 거기서 추출해야 함
-    // __m256i는 2개의 128비트 레인으로 구성됨
-    __m128i low_lane = _mm256_extracti128_si256(simd_int_vec, 0);  // 하위 128비트 (요소 0-3)
-    __m128i high_lane = _mm256_extracti128_si256(simd_int_vec, 1); // 상위 128비트 (요소 4-7)
-
-    // 128비트 레인에서 32비트 추출
-    int element0 = _mm_extract_epi32(low_lane, 0);  // 0 추출
-    int element3 = _mm_extract_epi32(low_lane, 3);  // 3 추출
-    int element4 = _mm_extract_epi32(high_lane, 0); // 4 추출
-    int element7 = _mm_extract_epi32(high_lane, 3); // 7 추출
-
-    std::cout << "추출된 요소들: " << element0 << ", " << element3 << ", "
-              << element4 << ", " << element7 << std::endl;
-
-    // 요소 삽입
-    // 삽입도 2단계: 128비트 레인 내에서 삽입 → 256비트로 재조립
-    __m128i new_low = _mm_insert_epi32(low_lane, 100, 1);  // 1을 100으로 교체
-    __m128i new_high = _mm_insert_epi32(high_lane, 200, 2); // 6을 200으로 교체
-
-    // 레인들을 다시 256비트 벡터로 결합
-    __m256i modified_int_vec = _mm256_setr_m128i(new_low, new_high);
-
-    print_m256i(modified_int_vec, "수정된 정수 벡터 (extract/insert)");
-
-    free(aligned_array);
-
-    return 0;
-}
diff --git a/01/import_simd/Makefile b/01/import_simd/Makefile
deleted file mode 100644
index ce35a5f..0000000
--- a/01/import_simd/Makefile
+++ /dev/null
@@ -1,23 +0,0 @@
-CXX=g++
-CXXFLAGS=-mavx2 -masm=att -std=c++11
-TARGET=simd_program
-BUILDDIR=build
-ASMFILE=$(BUILDDIR)/main.s
-SRCFILE=main.cpp
-OBJFILE=$(BUILDDIR)/main.o
-EXECUTABLE=$(BUILDDIR)/$(TARGET)
-
-all: $(EXECUTABLE)
-
-$(EXECUTABLE): $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) $(SRCFILE) -o $(EXECUTABLE)
-
-asm: $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) -S $(SRCFILE) -o $(ASMFILE)
-
-$(BUILDDIR):
-	mkdir -p $(BUILDDIR)
-
-clean:
-	rm -rf $(BUILDDIR) $(TARGET)
-
diff --git a/01/import_simd/README.md b/01/import_simd/README.md
deleted file mode 100644
index ef2cc88..0000000
--- a/01/import_simd/README.md
+++ /dev/null
@@ -1,126 +0,0 @@
-## SIMD (Single Instruction Multiple Data)
-
-일반연산: 1 + 1 = 2
-SIMD 연산: [1, 2, 3, 4] + [5, 6, 7, 8] = [6,8,10,12]
-
-### 1. Helper function
-
-- print_m256
-
-```cpp
-void print_m256(const __m256& v, const char * label) {
-    float result[8];
-    _mm256_storeu_ps(result, v); // SIMD register -> 일반 배열로 복사
-```
-
-- __m256: 256비트 레지스터 (32비트 float X 8개)
-- _mm256_storeu_ps: SIMD 값을 일반 배열로 복사
-
-### 2. Example 1: Float 연산 (8개 동시 처리)
-
-```cpp
-__m256 a = _mm256_set_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);
-__m256 b = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);
-```
-
-메모리 구조 (역순으로 저장)
-
-```cpp
-a: [8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0]
-b: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]
-
-__m256 sum = _mm256_add_ps(a, b);  // 8개를 한 번에 더함!
-```
-
-일반 방식 vs SIMD:
-
-```cpp
-// 일반 방식 (8번 반복)
-for (int i = 0; i < 8; i++) {
-    result[i] = a[i] + b[i];
-}
-
-// SIMD 방식 (1번 실행으로 8개 처리!)
-__m256 sum = _mm256_add_ps(a, b);
-```
-
-### 3. Example 2: Integer 연산 (8개 동시)
-
-```cpp
-__m256i int_a = _mm256_set_epi32(1, 2, 3, 4, 5, 6, 7, 8);
-//                               ↑ 역순으로 저장: [8,7,6,5,4,3,2,1]
-```
-- __m256i: 정수용 256비트 레지스터 (32비트 int × 8개)
-- _mm256_add_epi32: 8개 정수를 동시에 더함
-- _mm256_mullo_epi32: 8개 정수를 동시에 곱함
-
-### 4. Example 3: Double 연산 (4개 동시)
-
-```cpp
-__m256d double_a = _mm256_set_pd(1.0, 2.0, 3.0, 4.0);
-```
-- __m256d: Double용 256비트 레지스터 (64비트 double × 4개)
-- Double은 크기가 2배라서 4개만 들어감
-
-### 5. Example 4: 배열 합계
-
-```cpp
-float numbers[16] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
-
-__m256 vec1 = _mm256_loadu_ps(&numbers[0]);   // 앞 8개 로드
-__m256 vec2 = _mm256_loadu_ps(&numbers[8]);   // 뒤 8개 로드
-__m256 total = _mm256_add_ps(vec1, vec2);     // 8쌍 동시에 더하기
-```
-
-```
-vec1:  [1,  2,  3,  4,  5,  6,  7,  8]
-vec2:  [9, 10, 11, 12, 13, 14, 15, 16]
-       --------------------------------
-total: [10, 12, 14, 16, 18, 20, 22, 24]
-```
-
-Horizontal Sum (가로 합계)
-
-```cpp
-// total = [10, 12, 14, 16, 18, 20, 22, 24]
-// → 이걸 하나의 값으로 합치기
-
-__m128 high = _mm256_extractf128_ps(total, 1);  // 상위 4개
-__m128 low = _mm256_castps256_ps128(total);     // 하위 4개
-__m128 sum128 = _mm_add_ps(high, low);          // [28, 32, 36, 40]
-sum128 = _mm_hadd_ps(sum128, sum128);           // [60, 76, ...]
-sum128 = _mm_hadd_ps(sum128, sum128);           // [136, ...]
-```
-
---
-
-### 성능 비교
-
-```cpp
-// 일반 코드 (16번 더하기)
-for (int i = 0; i < 16; i++) sum += numbers[i];  // 16회 반복
-
-// SIMD 코드 (2번 + 알파)
-vec1 + vec2  // 8쌍 동시 처리
-+ horizontal sum  // 추가 연산
-```
-- 이론상: 최대 8배 빠름
-- 실제: 데이터 크기와 패턴에 따라 2~6배 정도
-
----
-
-### 주요 함수
-
-- _mm256_set_ps : 8개 float 값으로 레지스터 초기화
-- _mm256_add_ps : 8개 float 동시 덧셈
-- _mm256_mul_ps : 8개 float 동시 곱셈
-- _mm256_loadu_ps : 메모리에서 8개 float 로드 (정렬 불필요)
-- _mm256_storeu_ps : 레지스터를 메모리에 저장
-- _mm256_hadd_ps : 인접한 값들을 더함 (horizontal add)
-
-접미사 의미
-
-- _ps: Packed Single (float)
-- _pd: Packed Double
-- _epi32: 32bit integer
-
diff --git a/01/import_simd/main.cpp b/01/import_simd/main.cpp
deleted file mode 100644
index 6276e31..0000000
--- a/01/import_simd/main.cpp
+++ /dev/null
@@ -1,115 +0,0 @@
-#include <immintrin.h>
-#include <iostream>
-#include <iomanip>
-
-// Helper function to print __m256
-void print_m256(const __m256& v, const char* label) {
-    float result[8];
-    _mm256_storeu_ps(result, v);
-    std::cout << label << ": ";
-    for (int i = 7; i >= 0; i--) {
-        std::cout << std::setw(8) << result[i] << " ";
-    }
-    std::cout << std::endl;
-}
-
-// Helper function to print __m256i
-void print_m256i(const __m256i& v, const char* label) {
-    int result[8];
-    _mm256_storeu_si256((__m256i*)result, v);
-    std::cout << label << ": ";
-    for (int i = 7; i >= 0; i--) {
-        std::cout << std::setw(8) << result[i] << " ";
-    }
-    std::cout << std::endl;
-}
-
-// Helper function to print __m256d
-void print_m256d(const __m256d& v, const char* label) {
-    double result[4];
-    _mm256_storeu_pd(result, v);
-    std::cout << label << ": ";
-    for (int i = 3; i >= 0; i--) {
-        std::cout << std::setw(12) << result[i] << " ";
-    }
-    std::cout << std::endl;
-}
-
-int main() {
-    std::cout << "=== AVX2 SIMD Examples ===" << std::endl << std::endl;
-    
-    // Example 1: Float operations (8 floats at once)
-    std::cout << "--- Float Operations (8-wide) ---" << std::endl;
-    __m256 a = _mm256_set_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);
-    __m256 b = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);
-    
-    __m256 sum = _mm256_add_ps(a, b);
-    __m256 mul = _mm256_mul_ps(a, b);
-    __m256 sub = _mm256_sub_ps(a, b);
-    
-    print_m256(a, "A        ");
-    print_m256(b, "B        ");
-    print_m256(sum, "A + B    ");
-    print_m256(mul, "A * B    ");
-    print_m256(sub, "A - B    ");
-    
-    std::cout << std::endl;
-    
-    // Example 2: Integer operations (8 integers at once)
-    std::cout << "--- Integer Operations (8-wide) ---" << std::endl;
-    __m256i int_a = _mm256_set_epi32(1, 2, 3, 4, 5, 6, 7, 8);
-    __m256i int_b = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1);
-    __m256i int_sum = _mm256_add_epi32(int_a, int_b);
-    __m256i int_mul = _mm256_mullo_epi32(int_a, int_b);
-    
-    print_m256i(int_a, "A        ");
-    print_m256i(int_b, "B        ");
-    print_m256i(int_sum, "A + B    ");
-    print_m256i(int_mul, "A * B    ");
-    
-    std::cout << std::endl;
-    
-    // Example 3: Double operations (4 doubles at once)
-    std::cout << "--- Double Operations (4-wide) ---" << std::endl;
-    __m256d double_a = _mm256_set_pd(1.0, 2.0, 3.0, 4.0);
-    __m256d double_b = _mm256_set_pd(4.0, 3.0, 2.0, 1.0);
-    __m256d double_sum = _mm256_add_pd(double_a, double_b);
-    __m256d double_mul = _mm256_mul_pd(double_a, double_b);
-    
-    print_m256d(double_a, "A        ");
-    print_m256d(double_b, "B        ");
-    print_m256d(double_sum, "A + B    ");
-    print_m256d(double_mul, "A * B    ");
-    
-    std::cout << std::endl;
-    
-    // Example 4: Practical use case - sum array
-    std::cout << "--- Practical Example: Sum Array ---" << std::endl;
-    float numbers[16] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
-    
-    // SIMD version (process 8 at a time)
-    __m256 vec1 = _mm256_loadu_ps(&numbers[0]);
-    __m256 vec2 = _mm256_loadu_ps(&numbers[8]);
-    __m256 total = _mm256_add_ps(vec1, vec2);
-    
-    // Horizontal sum
-    __m128 high = _mm256_extractf128_ps(total, 1);
-    __m128 low = _mm256_castps256_ps128(total);
-    __m128 sum128 = _mm_add_ps(high, low);
-    sum128 = _mm_hadd_ps(sum128, sum128);
-    sum128 = _mm_hadd_ps(sum128, sum128);
-    
-    float final_sum;
-    _mm_store_ss(&final_sum, sum128);
-    
-    std::cout << "Sum of array (1-16): " << final_sum << std::endl;
-    
-    // Verify with scalar
-    float scalar_sum = 0;
-    for (int i = 0; i < 16; i++) {
-        scalar_sum += numbers[i];
-    }
-    std::cout << "Scalar verification: " << scalar_sum << std::endl;
-    
-    return 0;
-}
diff --git a/01/init_data/Makefile b/01/init_data/Makefile
deleted file mode 100644
index ce35a5f..0000000
--- a/01/init_data/Makefile
+++ /dev/null
@@ -1,23 +0,0 @@
-CXX=g++
-CXXFLAGS=-mavx2 -masm=att -std=c++11
-TARGET=simd_program
-BUILDDIR=build
-ASMFILE=$(BUILDDIR)/main.s
-SRCFILE=main.cpp
-OBJFILE=$(BUILDDIR)/main.o
-EXECUTABLE=$(BUILDDIR)/$(TARGET)
-
-all: $(EXECUTABLE)
-
-$(EXECUTABLE): $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) $(SRCFILE) -o $(EXECUTABLE)
-
-asm: $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) -S $(SRCFILE) -o $(ASMFILE)
-
-$(BUILDDIR):
-	mkdir -p $(BUILDDIR)
-
-clean:
-	rm -rf $(BUILDDIR) $(TARGET)
-
diff --git a/01/init_data/README.md b/01/init_data/README.md
deleted file mode 100644
index 5a32174..0000000
--- a/01/init_data/README.md
+++ /dev/null
@@ -1,107 +0,0 @@
-
-## SIMD 벡터를 초기화하는 4가지 방법과 성능 비교
-
-### 1. Zero Initialization (_mm256_setzero_*)
-
-```cpp
-__m256 simd_float_vec = _mm256_setzero_ps();
-// 결과: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
-```
-
-일반 방식 vs SIMD
-
-```cpp
-// 일반 (8번 반복)
-for (int i = 0; i < 8; i++) {
-    array[i] = 0.0f;
-}
-
-// SIMD (1개 명령어로 8개 동시에)
-vec = _mm256_setzero_ps();
-```
-
-### 2. Broadcast Initialization (_mm256_set1_*)
-
-```cpp
-__m256 vec = _mm256_set1_ps(42.0f);
-// 결과: [42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0]
-```
-
-- 모든 레인에 같은 값을 넣을 때 사용. 반복문 없이 한 번에 8개를 같은 값으로 설정.
-
-### 3. Individual Initialization (_mm256_set_*)
-
-```cpp
-__m256i vec = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1);
-// 주의: 역순! 결과: [1, 2, 3, 4, 5, 6, 7, 8]
-```
-
-- 첫 번째 인자(8)가 마지막 레인(인덱스 7)에 들어감
-- 마지막 인자(1)가 첫 번째 레인(인덱스 0)에 들어감
-
-### 4. Reverse Order Initialization (_mm256_setr_*)
-
-```cpp
-__m256 vec = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);
-// 결과: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]
-```
-
-- 첫 번째 인자가 첫 번째 레인에
-- 마지막 인자가 마지막 레인에
-
----
-
-### 성능 비교
-
-
-```cpp
-// 100만 번 반복해서 시간 측정
-for (int i = 0; i < 1000000; i++) {
-    // 일반 방식
-    for (int j = 0; j < 8; j++) {
-        array[j] = value;
-    }
-}
-// vs
-for (int i = 0; i < 1000000; i++) {
-    // SIMD 방식 (1개 명령어)
-    vec = _mm256_set1_ps(value);
-}
-```
-
----
-
-### 사용 예시
-
-```cpp
-// 배열 0으로 초기화
-__m256 zeros = _mm256_setzero_ps();
-for (int i = 0; i < array_size; i += 8) {
-    _mm256_storeu_ps(&array[i], zeros);
-}
-
-// 배열을 특정 값으로 채우기
-__m256 fill_value = _mm256_set1_ps(3.14f);
-for (int i = 0; i < array_size; i += 8) {
-    _mm256_storeu_ps(&array[i], fill_value);
-}
-
-// 1, 2, 3, 4, 5, 6, 7, 8로 초기화
-__m256 sequence = _mm256_setr_ps(1, 2, 3, 4, 5, 6, 7, 8);
-```
-
----
-
-### Summery
-
-타입별 함수 구분
-
-- _ps: float
-- _pd: double
-- _epi32: int32
-- _epi16: int16
-
-성능 차이
-
-- Zero/Broadcast: 매우 빠름 (1개 명령어)
-- Individual: 약간 느림 (여러 명령어 조합)
diff --git a/01/init_data/main.cpp b/01/init_data/main.cpp
deleted file mode 100644
index 9748057..0000000
--- a/01/init_data/main.cpp
+++ /dev/null
@@ -1,224 +0,0 @@
-#include "../../include/simd_utils.h"
-#include <chrono>
-#include <iostream>
-#include <iomanip>
-
-// Constants
-constexpr int NUM_ITERATIONS = 1000000;
-
-template <typename T, size_t N>
-void printArray(const T (&arr)[N], const std::string &description) {
-    std::cout << description << ": ";
-    for (size_t i = 0; i < N; ++i) {
-        std::cout << arr[i] << ", ";
-    }
-    std::cout << std::endl;
-}
-
-void copyFromSIMD(float* dest, const __m256& src) {
-    _mm256_storeu_ps(dest, src);
-}
-
-void copyFromSIMD(double* dest, const __m256d& src) {
-    _mm256_storeu_pd(dest, src);
-}
-
-void copyFromSIMD(int* dest, const __m256i& src) {
-    _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest), src);
-}
-
-void copyFromSIMD(short* dest, const __m256i& src) {
-    _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest), src);
-}
-
-int main() {
-    std::cout << "=== SIMD Data Initialization Methods ===" << std::endl;
-    std::cout << std::endl;
-
-    // --------- 1. Zero Initialization (_mm256_setzero_*) -------------
-    std::cout << "1. Zero Initialization (_mm256_setzero_*)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "Initializes all elements of a SIMD vector to zero." << std::endl;
-    std::cout << std::endl;
-
-    // Standard method for float array
-    float std_float_array[8];
-    auto start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        for (int lane = 0; lane < 8; ++lane) {
-            std_float_array[lane] = 0.0f;
-        }
-    }
-    auto stop = std::chrono::high_resolution_clock::now();
-    auto duration_std = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // SIMD method for float vector
-    __m256 simd_float_vec;
-    start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        simd_float_vec = _mm256_setzero_ps();
-    }
-    stop = std::chrono::high_resolution_clock::now();
-    auto duration_simd = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // Print results
-    std::cout << "Float Zero Initialization:" << std::endl;
-    std::cout << "  Standard method: " << duration_std.count() << " microseconds" << std::endl;
-    std::cout << "  SIMD method:     " << duration_simd.count() << " microseconds" << std::endl;
-    std::cout << "  Speedup:         " << std::fixed << std::setprecision(2)
-              << static_cast<double>(duration_std.count()) / duration_simd.count() << "x" << std::endl;
-
-    // Print the SIMD vector
-    print_m256(simd_float_vec, "Zero-initialized float vector");
-
-    // Also demonstrate zero initialization for integers and doubles
-    __m256i simd_int_vec = _mm256_setzero_si256();
-    __m256d simd_double_vec = _mm256_setzero_pd();
-
-    print_m256i(simd_int_vec, "Zero-initialized integer vector");
-    print_m256d(simd_double_vec, "Zero-initialized double vector");
-    std::cout << std::endl;
-
-    // --------- 2. Broadcast Initialization (_mm256_set1_*) -------------
-    std::cout << "2. Broadcast Initialization (_mm256_set1_*)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "Initializes all elements of a SIMD vector to the same value." << std::endl;
-    std::cout << std::endl;
-
-    // Standard method for double array
-    double std_double_array[4];
-    start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        for (int lane = 0; lane < 4; ++lane) {
-            std_double_array[lane] = 10.0;
-        }
-    }
-    stop = std::chrono::high_resolution_clock::now();
-    duration_std = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // SIMD method for double vector
-    __m256d simd_double_vec2;
-    start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        simd_double_vec2 = _mm256_set1_pd(10.0);
-    }
-    stop = std::chrono::high_resolution_clock::now();
-    duration_simd = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // Print results
-    std::cout << "Double Broadcast Initialization:" << std::endl;
-    std::cout << "  Standard method: " << duration_std.count() << " microseconds" << std::endl;
-    std::cout << "  SIMD method:     " << duration_simd.count() << " microseconds" << std::endl;
-    std::cout << "  Speedup:         " << std::fixed << std::setprecision(2)
-              << static_cast<double>(duration_std.count()) / duration_simd.count() << "x" << std::endl;
-
-    // Print the SIMD vector
-    print_m256d(simd_double_vec2, "Broadcast-initialized double vector (10.0)");
-
-    // Also demonstrate broadcast initialization for floats and integers
-    __m256 simd_float_vec2 = _mm256_set1_ps(42.0f);
-    __m256i simd_int_vec2 = _mm256_set1_epi32(100);
-
-    print_m256(simd_float_vec2, "Broadcast-initialized float vector (42.0)");
-    print_m256i(simd_int_vec2, "Broadcast-initialized integer vector (100)");
-    std::cout << std::endl;
-
-    // --------- 3. Individual Element Initialization (_mm256_set_*) -------------
-    std::cout << "3. Individual Element Initialization (_mm256_set_*)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "Initializes each element of a SIMD vector individually." << std::endl;
-    std::cout << "Note: Elements are specified in reverse order (high to low)." << std::endl;
-    std::cout << std::endl;
-
-    // Standard method for int array
-    int std_int_array[8];
-    start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        for (int lane = 0; lane < 8; ++lane) {
-            std_int_array[lane] = lane + 1;
-        }
-    }
-    stop = std::chrono::high_resolution_clock::now();
-    duration_std = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // SIMD method for int vector
-    __m256i simd_int_vec3;
-    start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        // Note: _mm256_set_epi32 takes arguments in reverse order (high to low)
-        simd_int_vec3 = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1);
-    }
-    stop = std::chrono::high_resolution_clock::now();
-    duration_simd = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // Print results
-    std::cout << "Integer Individual Initialization:" << std::endl;
-    std::cout << "  Standard method: " << duration_std.count() << " microseconds" << std::endl;
-    std::cout << "  SIMD method:     " << duration_simd.count() << " microseconds" << std::endl;
-    std::cout << "  Speedup:         " << std::fixed << std::setprecision(2)
-              << static_cast<double>(duration_std.count()) / duration_simd.count() << "x" << std::endl;
-
-    // Print the SIMD vector
-    print_m256i(simd_int_vec3, "Individually-initialized integer vector");
-
-    // Also demonstrate individual initialization for floats and doubles
-    __m256 simd_float_vec3 = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);
-    __m256d simd_double_vec3 = _mm256_set_pd(4.0, 3.0, 2.0, 1.0);
-
-    print_m256(simd_float_vec3, "Individually-initialized float vector");
-    print_m256d(simd_double_vec3, "Individually-initialized double vector");
-    std::cout << std::endl;
-
-    // --------- 4. Reverse Order Initialization (_mm256_setr_*) -------------
-    std::cout << "4. Reverse Order Initialization (_mm256_setr_*)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "Initializes each element of a SIMD vector individually in natural order." << std::endl;
-    std::cout << "Note: Elements are specified in natural order (low to high)." << std::endl;
-    std::cout << std::endl;
-
-    // Standard method for short array
-    short std_short_array[16];
-    start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        for (int lane = 0; lane < 16; ++lane) {
-            std_short_array[lane] = static_cast<short>(lane + 1);
-        }
-    }
-    stop = std::chrono::high_resolution_clock::now();
-    duration_std = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // SIMD method for short vector
-    __m256i simd_short_vec;
-    start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < NUM_ITERATIONS; ++i) {
-        // Note: _mm256_setr_epi16 takes arguments in natural order (low to high)
-        simd_short_vec = _mm256_setr_epi16(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16);
-    }
-    stop = std::chrono::high_resolution_clock::now();
-    duration_simd = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
-
-    // Print results
-    std::cout << "Short Reverse Order Initialization:" << std::endl;
-    std::cout << "  Standard method: " << duration_std.count() << " microseconds" << std::endl;
-    std::cout << "  SIMD method:     " << duration_simd.count() << " microseconds" << std::endl;
-    std::cout << "  Speedup:         " << std::fixed << std::setprecision(2)
-              << static_cast<double>(duration_std.count()) / duration_simd.count() << "x" << std::endl;
-
-    // Print the SIMD vector (first 8 elements)
-    // Note: We need to extract the shorts from the __m256i
-    short short_array[16];
-    _mm256_storeu_si256(reinterpret_cast<__m256i*>(short_array), simd_short_vec);
-
-    std::cout << "Reverse-initialized short vector: [";
-    for (int i = 0; i < 15; i++) {
-        std::cout << short_array[i] << ", ";
-    }
-    std::cout << short_array[15] << "]" << std::endl;
-
-    // Also demonstrate reverse initialization for floats
-    __m256 simd_float_vec4 = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);
-    print_m256(simd_float_vec4, "Reverse-initialized float vector");
-
-    return 0;
-}
-
diff --git a/01/loading_data/Makefile b/01/loading_data/Makefile
deleted file mode 100644
index ce35a5f..0000000
--- a/01/loading_data/Makefile
+++ /dev/null
@@ -1,23 +0,0 @@
-CXX=g++
-CXXFLAGS=-mavx2 -masm=att -std=c++11
-TARGET=simd_program
-BUILDDIR=build
-ASMFILE=$(BUILDDIR)/main.s
-SRCFILE=main.cpp
-OBJFILE=$(BUILDDIR)/main.o
-EXECUTABLE=$(BUILDDIR)/$(TARGET)
-
-all: $(EXECUTABLE)
-
-$(EXECUTABLE): $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) $(SRCFILE) -o $(EXECUTABLE)
-
-asm: $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) -S $(SRCFILE) -o $(ASMFILE)
-
-$(BUILDDIR):
-	mkdir -p $(BUILDDIR)
-
-clean:
-	rm -rf $(BUILDDIR) $(TARGET)
-
diff --git a/01/loading_data/README.md b/01/loading_data/README.md
deleted file mode 100644
index 223f107..0000000
--- a/01/loading_data/README.md
+++ /dev/null
@@ -1,238 +0,0 @@
-
-## SIMD 데이터 로딩과 저장
-
-### 1. 정렬된 vs 정렬되지 않은 메모리
-
-- 메모리 정렬이란
-    ```
-    정렬된 주소 (32바이트 정렬):
-    0x0000  ✓ 32의 배수
-    0x0020  ✓
-    x0040  ✓
-
-    정렬 안 된 주소:
-    0x0001  ✗ 32의 배수 아님
-    0x0023  ✗
-    ```
-- CPU가 메모리를 읽는 방식
-    정렬된 로드
-    ```
-    메모리 주소 0x1000 (32바이트 정렬)
-    ┌────────────────────────────────┐
-    │   32바이트를 한 번에 읽기      │
-    └────────────────────────────────┘
-    CPU 메모리 버스와 정확히 일치 → 1번 읽기
-    ```
-    정렬 안 된 로드
-    ```
-    메모리 주소 0x1004 (정렬 안 됨)
-        ┌────────────────────────────────┐
-        │   경계를 넘어감                │
-        └────────────────────────────────┘
-    ┌─────────────┐         ┌──────────────┐
-    │  1번째 읽기 │         │  2번째 읽기  │
-    └─────────────┘         └──────────────┘
-             ↓                      ↓
-        [결과 조합] → 느림
-    ```
-
-### 2. 마스크 로드/저장
-
-- 마스크의 작동 원리
-    ```
-    __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);
-    ```
-- 비트 레벨
-    ```
-    -1 = 0xFFFFFFFF (모든 비트 1) → 로드/저장
-     0 = 0x00000000 (모든 비트 0) → 무시
-
-    마스크:  [  0  ][-1 ][ 0 ][-1 ][ 0 ][-1 ][ 0 ][-1 ]
-    인덱스:    7     6     5    4     3    2     1    0
-    ```
-- 마스크 로드
-    ```
-    메모리:    [1] [2] [3] [4] [5] [6] [7] [8]
-    마스크:     0  -1   0  -1   0  -1   0  -1
-               ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓
-    결과:      [0] [2] [0] [4] [0] [6] [0] [8]
-    ```
-
-사용 예
-
-```cpp
-float data[11];  // 11개 요소
-
-// 처음 8개는 일반 SIMD로 처리
-_mm256_store_ps(&data[0], vec1);
-
-// 마지막 3개 처리
-__m256 last_vec = /* 마지막 데이터 */;
-__m256i mask = _mm256_set_epi32(0, 0, 0, 0, 0, -1, -1, -1);
-//                              [7][6][5][4][3][2][1][0]
-//                               ✗  ✗  ✗  ✗  ✗  ✓  ✓  ✓
-
-_mm256_maskstore_ps(&data[8], mask, last_vec);
-// data[8]부터 8개 위치를 시도하지만
-// 마스크 때문에 실제로는 data[8], data[9], data[10]만 저장
-// data[11], [12], [13], [14], [15]는 저장 안 함 (범위 밖)
-```
-
-실제 동작
-
-```
-메모리 위치:  [8]  [9]  [10] [11] [12] [13] [14] [15]
-마스크:       -1   -1   -1    0    0    0    0    0
-             ✓    ✓    ✓    ✗    ✗    ✗    ✗    ✗
-결과:        저장  저장  저장  무시  무시  무시  무시  무시
-```
-
-사용예시
-
-```cpp
-void process_array(float* data, int size) {
-    int i = 0;
-
-    // 8개씩 처리
-    for (; i + 8 <= size; i += 8) {
-        __m256 vec = _mm256_load_ps(&data[i]);
-        // 처리...
-        _mm256_store_ps(&data[i], result);
-    }
-
-    // 남은 요소 처리 (8개 미만)
-    if (i < size) {
-        int remaining = size - i;
-
-        // 남은 개수만큼 마스크 생성
-        int mask_values[8] = {0};
-        for (int j = 0; j < remaining; j++) {
-            mask_values[j] = -1;
-        }
-
-        __m256i mask = _mm256_set_epi32(
-            mask_values[7], mask_values[6],
-            mask_values[5], mask_values[4],
-            mask_values[3], mask_values[2],
-            mask_values[1], mask_values[0]
-        );
-
-        __m256 vec = _mm256_maskload_ps(&data[i], mask);
-        // 처리...
-        _mm256_maskstore_ps(&data[i], mask, result);
-    }
-}
-```
-
-### 3. 스트림 연산 (Non-temporal)
-
-CPU 캐시 계층 구조
-
-```
-CPU
- ├─ L1 캐시 (32KB, 매우 빠름)
- ├─ L2 캐시 (256KB, 빠름)
- ├─ L3 캐시 (8MB, 보통)
- └─ RAM (16GB, 느림)
-```
-
-일반 로드/저장
-
-```
-일반 저장:
-CPU → L1 캐시 → L2 캐시 → L3 캐시 → RAM
-         ↓
-    캐시에 복사본 유지 (다음에 빠르게 접근 가능)
-```
-
-문제: 대용량 데이터를 한 번만 처리할 때
-
-```
-1GB 데이터 처리 → 8MB 캐시를 모두 차지
-→ 다른 중요한 데이터가 캐시에서 밀려남 (캐시 오염)
-```
-
-스트림 저장
-
-```
-스트림 저장:
-CPU → RAM (캐시 우회)
-       ↓
-   캐시를 건드리지 않음
-```
-
-다음 경우에 사용
-- 대용량 파일 읽기/쓰기
-- 비디오 프레임 처리 (한 번만 사용)
-- 대용량 배열 초기화
-
-코드 예시
-
-```cpp
-// 1GB 배열 초기화
-for (int i = 0; i < size; i += 8) {
-    __m256 zero = _mm256_setzero_ps();
-    _mm256_stream_ps(&array[i], zero);  // 캐시 우회
-}
-_mm_sfence();  // 모든 쓰기 완료 대기
-```
-
-_mm_sfence()의 역할
-
-스트림 저장은 비동기적으로 실행
-
-```
-CPU: "저장해!" → 계속 다음 코드 실행
-                  ↓
-            백그라운드에서 저장 중...
-
-_mm_sfence(): "모든 저장이 완료될 때까지 기다려!"
-```
-
-성능 비교 요약
-
-```
-연산 타입              속도        사용 시기
-━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
-정렬된 로드/저장      매우 빠름    항상 (가능하면)
-정렬 안 된 로드/저장  약간 느림    정렬 불가능할 때
-마스크 로드/저장      중간         일부 요소만 처리
-스트림 로드/저장      빠름         대용량 1회 처리
-```
-
-시나리오
-
-- 시나리오 1: 이미지 처리 (재사용)
-    ```cpp
-    // 이미지를 여러 번 처리 → 일반 로드/저장 (캐시 활용)
-    for (int pass = 0; pass < 10; pass++) {
-        for (int i = 0; i < size; i += 8) {
-            __m256 pixel = _mm256_load_ps(&image[i]);
-            // 처리...
-            _mm256_store_ps(&image[i], result);
-        }
-    }
-    ```
-
-- 시나리오 2: 비디오 인코딩 (1회만)
-    ```cpp
-    // 프레임을 한 번만 처리 → 스트림 저장 (캐시 오염 방지)
-    for (int i = 0; i < frame_size; i += 8) {
-        __m256 frame = _mm256_loadu_ps(&input[i]);
-        __m256 encoded = encode(frame);
-        _mm256_stream_ps(&output[i], encoded);  // 캐시 우회
-    }
-    _mm_sfence();
-    ```
-
-- 시나리오 3: 배열 끝 처리
-    ```cpp
-    // 크기가 8의 배수가 아닐 때 → 마스크 사용
-    int size = 27;  // 8*3 + 3
-    for (int i = 0; i < 24; i += 8) {
-        // 일반 SIMD
-    }gg
-    // 마지막 3개는 마스크로
-    __m256i mask = _mm256_set_epi32(0,0,0,0,0,-1,-1,-1);
-    _mm256_maskstore_ps(&array[24], mask, last_vec);
-    ```
diff --git a/01/loading_data/main.cpp b/01/loading_data/main.cpp
deleted file mode 100644
index b5aa601..0000000
--- a/01/loading_data/main.cpp
+++ /dev/null
@@ -1,220 +0,0 @@
-#include "../../include/simd_utils.h"
-#include <iostream>
-#include <iomanip>
-#include <memory>
-
-/**
- * SIMD 데이터 로딩과 저장
- *
- * 이 예제는 SIMD 벡터로 데이터를 로드하는 여러 방법을 보여줍니다:
- * 1. 정렬된 로드 (_mm256_load_ps) - 32바이트 정렬 메모리 필요
- * 2. 정렬되지 않은 로드 (_mm256_loadu_ps) - 모든 메모리 주소에서 작동
- * 3. 마스크 로드 (_mm256_maskload_ps) - 마스크를 기반으로 선택적 로드
- * 4. 스트림 로드 (_mm256_stream_load_si256) - 캐시를 우회하는 논-템포럴 로드
- *
- * SIMD 데이터를 저장하는 방법
- * 1. 정렬된 저장 (_mm256_store_ps) - 32바이트 정렬 메모리 필요
- * 2. 정렬되지 않은 저장 (_mm256_storeu_ps) - 모든 메모리 주소에서 작동
- * 3. 마스크 저장 (_mm256_maskstore_ps) - 마스크를 기반으로 선택적 저장
- * 4. 스트림 저장 (_mm256_stream_ps) - 캐시를 우회하는 논-템포럴 저장
- *
- */
-
-const int ARRAY_SIZE = 8;
-const int TEST_ITERATIONS = 10000000;  // 1천만 번 반복 (성능 측정용)
-
-int main() {
-    std::cout << "=== SIMD 데이터 로딩과 저장 ===" << std::endl;
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 1. 정렬된 vs. 정렬되지 않은 로드
-    // ========================================================================
-    std::cout << "1. 정렬된 vs. 정렬되지 않은 로드" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "정렬된 메모리와 정렬되지 않은 메모리 접근 비교" << std::endl;
-    std::cout << std::endl;
-
-    // 정렬된 메모리와 정렬되지 않은 메모리 할당
-    float* aligned_data = aligned_alloc<float>(ARRAY_SIZE, 32);  // AVX용 32바이트 정렬
-    float* unaligned_data = new float[ARRAY_SIZE + 1];           // +1로 정렬 안 된 포인터 생성 가능
-    float* unaligned_ptr = unaligned_data + 1;                   // 1칸 오프셋으로 정렬 깨기
-
-    // 데이터 초기화
-    for (int i = 0; i < ARRAY_SIZE; i++) {
-        aligned_data[i] = static_cast<float>(i + 1);
-        unaligned_ptr[i] = static_cast<float>(i + 1);
-    }
-
-    // 정렬된 로드 시연
-    // 32바이트 정렬된 메모리에서만 사용 가능 (더 빠름)
-    __m256 aligned_vec = _mm256_load_ps(aligned_data);
-    print_m256(aligned_vec, "정렬된 로드 결과");
-
-    // 정렬되지 않은 로드 시연
-    // 모든 주소에서 작동하지만 약간 느림
-    __m256 unaligned_vec = _mm256_loadu_ps(unaligned_ptr);
-    print_m256(unaligned_vec, "정렬되지 않은 로드 결과");
-
-    // 성능 비교
-    Timer timer("정렬된 vs. 정렬되지 않은 로드 성능");
-
-    // 정렬된 로드 벤치마크
-    auto aligned_load = [&]() {
-        __m256 result;
-        for (int i = 0; i < TEST_ITERATIONS; i++) {
-            result = _mm256_load_ps(aligned_data);  // 정렬된 로드
-        }
-        return result;
-    };
-
-    // 정렬되지 않은 로드 벤치마크
-    auto unaligned_load = [&]() {
-        __m256 result;
-        for (int i = 0; i < TEST_ITERATIONS; i++) {
-            result = _mm256_loadu_ps(unaligned_ptr);  // 정렬 안 된 로드
-        }
-        return result;
-    };
-
-    benchmark_comparison("로드 연산", aligned_load, unaligned_load, 10);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 2. 마스크 로드
-    // ========================================================================
-    std::cout << "2. 마스크 로드" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "마스크를 기반으로 선택적으로 요소 로드하기" << std::endl;
-    std::cout << std::endl;
-
-    // 요소 0, 2, 4, 6만 로드하는 마스크 생성
-    // -1 (모든 비트 1) = 로드, 0 = 로드 안 함
-    __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);
-
-    // 마스크 로드 수행 (마스크로 선택되지 않은 요소는 0이 됨)
-    __m256 masked_vec = _mm256_maskload_ps(aligned_data, mask);
-    print_m256(masked_vec, "마스크 로드 결과 (짝수 인덱스만)");
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 3. 정렬된 vs. 정렬되지 않은 저장
-    // ========================================================================
-    std::cout << "3. 정렬된 vs. 정렬되지 않은 저장" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "정렬된 저장과 정렬되지 않은 저장 연산 비교" << std::endl;
-    std::cout << std::endl;
-
-    // 테스트용 벡터 생성
-    __m256 test_vec = _mm256_set_ps(16.0f, 14.0f, 12.0f, 10.0f, 8.0f, 6.0f, 4.0f, 2.0f);
-
-    // 정렬된 저장 수행
-    _mm256_store_ps(aligned_data, test_vec);
-
-    std::cout << "정렬된 저장 결과: [";
-    for (int i = 0; i < ARRAY_SIZE - 1; i++) {
-        std::cout << aligned_data[i] << ", ";
-    }
-    std::cout << aligned_data[ARRAY_SIZE - 1] << "]" << std::endl;
-
-    // 정렬되지 않은 저장 수행
-    _mm256_storeu_ps(unaligned_ptr, test_vec);
-
-    std::cout << "정렬되지 않은 저장 결과: [";
-    for (int i = 0; i < ARRAY_SIZE - 1; i++) {
-        std::cout << unaligned_ptr[i] << ", ";
-    }
-    std::cout << unaligned_ptr[ARRAY_SIZE - 1] << "]" << std::endl;
-
-    // 성능 비교
-    Timer timer2("정렬된 vs. 정렬되지 않은 저장 성능");
-
-    // 정렬된 저장 벤치마크
-    auto aligned_store = [&]() {
-        for (int i = 0; i < TEST_ITERATIONS; i++) {
-            _mm256_store_ps(aligned_data, test_vec);  // 정렬된 저장
-        }
-    };
-
-    // 정렬되지 않은 저장 벤치마크
-    auto unaligned_store = [&]() {
-        for (int i = 0; i < TEST_ITERATIONS; i++) {
-            _mm256_storeu_ps(unaligned_ptr, test_vec);  // 정렬 안 된 저장
-        }
-    };
-
-    benchmark_comparison("저장 연산", aligned_store, unaligned_store, 10);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 4. 마스크 저장
-    // ========================================================================
-    std::cout << "4. 마스크 저장" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "마스크를 기반으로 선택적으로 요소 저장하기" << std::endl;
-    std::cout << std::endl;
-
-    // 정렬된 데이터 초기화
-    for (int i = 0; i < ARRAY_SIZE; i++) {
-        aligned_data[i] = 0.0f;
-    }
-
-    // 요소 1, 3, 5, 7만 저장하는 마스크 생성
-    __m256i mask2 = _mm256_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0);
-
-    // 마스크 저장 수행 (마스크가 0인 위치는 변경 안 됨)
-    _mm256_maskstore_ps(aligned_data, mask2, test_vec);
-
-    std::cout << "마스크 저장 결과 (홀수 인덱스만): [";
-    for (int i = 0; i < ARRAY_SIZE - 1; i++) {
-        std::cout << aligned_data[i] << ", ";
-    }
-    std::cout << aligned_data[ARRAY_SIZE - 1] << "]" << std::endl;
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 5. 스트림 로드/저장 (논-템포럴)
-    // ========================================================================
-    std::cout << "5. 스트림 로드/저장 (논-템포럴)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "캐시를 우회하는 논-템포럴 로드와 저장 사용" << std::endl;
-    std::cout << "곧 재사용되지 않을 대용량 데이터에 유용합니다." << std::endl;
-    std::cout << std::endl;
-
-    // 스트리밍 연산을 보여줄 대용량 배열 할당
-    const int LARGE_SIZE = 1024;
-    float* large_array = aligned_alloc<float>(LARGE_SIZE, 32);
-
-    // 배열 초기화
-    for (int i = 0; i < LARGE_SIZE; i++) {
-        large_array[i] = static_cast<float>(i);
-    }
-
-    // 스트림 로드와 저장 수행
-    for (int i = 0; i < LARGE_SIZE; i += 8) {
-        // 스트림 로드 (캐스팅이 필요한 _mm256_stream_load_si256 사용)
-        __m256 loaded = _mm256_loadu_ps(&large_array[i]);
-
-        // 데이터 처리 (간단하게 2를 곱함)
-        __m256 processed = _mm256_mul_ps(loaded, _mm256_set1_ps(2.0f));
-
-        // 스트림 저장 (캐시를 우회하는 논-템포럴 저장)
-        // 대용량 데이터를 한 번만 쓸 때 캐시 오염 방지
-        _mm256_stream_ps(&large_array[i], processed);
-    }
-
-    // 모든 스트리밍 저장이 완료되도록 보장 (메모리 펜스)
-    _mm_sfence();
-
-    std::cout << "스트림 저장 결과 (처음 16개 요소): [";
-    for (int i = 0; i < 15; i++) {
-        std::cout << large_array[i] << ", ";
-    }
-    std::cout << large_array[15] << "]" << std::endl;
-
-    free(aligned_data);
-    delete[] unaligned_data;
-    free(large_array);
-
-    return 0;
-}
diff --git a/02/simple_maths/Makefile b/02/simple_maths/Makefile
deleted file mode 100644
index b8c42d7..0000000
--- a/02/simple_maths/Makefile
+++ /dev/null
@@ -1,23 +0,0 @@
-CXX=g++
-CXXFLAGS=-mavx2 -mfma -masm=att -std=c++11
-TARGET=simd_program
-BUILDDIR=build
-ASMFILE=$(BUILDDIR)/main.s
-SRCFILE=main.cpp
-OBJFILE=$(BUILDDIR)/main.o
-EXECUTABLE=$(BUILDDIR)/$(TARGET)
-
-all: $(EXECUTABLE)
-
-$(EXECUTABLE): $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) $(SRCFILE) -o $(EXECUTABLE)
-
-asm: $(SRCFILE) | $(BUILDDIR)
-	$(CXX) $(CXXFLAGS) -S $(SRCFILE) -o $(ASMFILE)
-
-$(BUILDDIR):
-	mkdir -p $(BUILDDIR)
-
-clean:
-	rm -rf $(BUILDDIR) $(TARGET)
-
diff --git a/02/simple_maths/README.md b/02/simple_maths/README.md
deleted file mode 100644
index 5f15797..0000000
--- a/02/simple_maths/README.md
+++ /dev/null
@@ -1,138 +0,0 @@
-
-## SIMD의 8가지 수학 연산
-
-수평 연산이 느린 이유는 데이터를 재배치해야 하기 때문
-
-가능하면 수직 연산을 사용하는 게 좋다.
-
-### 1-4. 기본 산술 연산 (수직 연산)
-
-덧셈, 뺄셈, 곱셈, 나눗셈 모두 동일한 패턴
-
-```cpp
-__m256 result = _mm256_add_ps(vector1, vector2);
-```
-
-수직 연산
-
-```
-vector1: [1, 2, 3, 4, 5, 6, 7, 8]
-vector2: [8, 7, 6, 5, 4, 3, 2, 1]
-         ↓  ↓  ↓  ↓  ↓  ↓  ↓  ↓  (같은 위치끼리)
-결과:    [9, 9, 9, 9, 9, 9, 9, 9]
-```
-
-### 5. FMA (Fused Multiply-Add)
-
-```cpp
-__m256 result = _mm256_fmadd_ps(vector1, vector2, vector3);
-// result = vector1 * vector2 + vector3
-```
-
-일반 방식 vs FMA
-
-```
-일반:
-1. temp = vector1 * vector2  (곱셈)
-2. result = temp + vector3   (덧셈)
-
-FMA:
-1. result = vector1 * vector2 + vector3  (한 번에)
-```
-
-정확도 차이
-
-```
-// 일반 방식
-float a = 1e20f, b = 1e-20f, c = 1.0f;
-float temp = a * b;  // 1.0 (반올림 발생)
-float result = temp + c;  // 2.0
-
-// FMA 방식
-// 내부에서 더 높은 정밀도로 계산
-// 최종 결과만 반올림 → 더 정확
-```
-
-### 6. 제곱근
-
-```cpp
-__m256 sqrt_result = _mm256_sqrt_ps(pos_vector);
-```
-
-8개의 제곱근을 동시에 계산
-
-```
-입력:  [1,  4,  9, 16, 25, 36, 49, 64]
-결과:  [1,  2,  3,  4,  5,  6,  7,  8]
-```
-
-### 7. 최소/최대
-
-```cpp
-__m256 min = _mm256_min_ps(v1, v2);
-__m256 max = _mm256_max_ps(v1, v2);
-```
-
-요소별 비교
-
-```
-v1:  [1, 2, 3, 4, 5, 6, 7, 8]
-v2:  [8, 7, 6, 5, 4, 3, 2, 1]
-     ↓  ↓  ↓  ↓  ↓  ↓  ↓  ↓
-min: [1, 2, 3, 4, 4, 3, 2, 1]
-max: [8, 7, 6, 5, 5, 6, 7, 8]
-```
-
-### 8. 수평 연산 (특이함!)
-
-일반 연산 (수직)
-
-```
-v1: [1, 2, 3, 4, 5, 6, 7, 8]
-v2: [9,10,11,12,13,14,15,16]
-    ↓  ↓  ↓  ↓  ↓  ↓  ↓  ↓  (같은 인덱스끼리)
-```
-
-수평 덧셈 (hadd)
-
-```
-v1: [1, 2, 3, 4 | 5, 6, 7, 8]
-     └┬┘ └┬┘      └┬┘ └┬┘
-      3   7        11  15
-
-v2: [9,10,11,12 |13,14,15,16]
-     └┬┘ └┬┘      └┬┘ └┬┘
-     19  23       27  31
-
-결과 = _mm256_hadd_ps(v1, v2):
-[3, 7, 19, 23 | 11, 15, 27, 31]
- ↑  ↑   ↑   ↑    ↑   ↑   ↑   ↑
-v1의 v1의 v2의 v2의 v1의 v1의 v2의 v2의
-인접 인접 인접 인접 인접 인접 인접 인접
-```
-
-- 하위 128비트: v1[0]+v1[1], v1[2]+v1[3], v2[0]+v2[1], v2[2]+v2[3]
-- 상위 128비트: v1[4]+v1[5], v1[6]+v1[7], v2[4]+v2[5], v2[6]+v2[7]
-
-```cpp
-// 내적(dot product) 계산
-__m256 prod = _mm256_mul_ps(a, b);  // [a0*b0, a1*b1, ...]
-__m256 sum1 = _mm256_hadd_ps(prod, prod);
-__m256 sum2 = _mm256_hadd_ps(sum1, sum1);
-// 최종적으로 모든 요소의 합을 구함
-```
-
----
-
-### 성능 특성
-
-```
-연산         상대 속도
-덧셈/뺄셈    매우 빠름 (1 사이클)
-곱셈         빠름 (3-5 사이클)
-나눗셈       느림 (10-20 사이클)
-FMA          곱셈과 비슷 (3-5 사이클)
-제곱근       중간 (10-15 사이클)
-min/max      매우 빠름 (1-2 사이클)
-hadd/hsub    느림 (5-10 사이클) - 데이터 재배치 필요
-```
diff --git a/02/simple_maths/main.cpp b/02/simple_maths/main.cpp
deleted file mode 100644
index c3db7e7..0000000
--- a/02/simple_maths/main.cpp
+++ /dev/null
@@ -1,275 +0,0 @@
-#include "../../include/simd_utils.h"
-#include <iostream>
-#include <iomanip>
-#include <cmath>
-
-/**
- * SIMD 기본 수학 연산
- *
- * 1. 덧셈 (_mm256_add_ps)
- * 2. 뺄셈 (_mm256_sub_ps)
- * 3. 곱셈 (_mm256_mul_ps)
- * 4. 나눗셈 (_mm256_div_ps)
- * 5. 융합 곱셈-덧셈 (_mm256_fmadd_ps)
- * 6. 제곱근 (_mm256_sqrt_ps)
- * 7. 최소/최대 (_mm256_min_ps, _mm256_max_ps)
- * 8. 수평 연산 (_mm256_hadd_ps, _mm256_hsub_ps)
- *
- * 각 연산마다 SIMD와 스칼라 구현의 성능을 비교
- */
-
-int main() {
-    std::cout << "=== SIMD 수학 연산 ===" << std::endl;
-    std::cout << std::endl;
-
-    // 테스트 데이터 초기화
-    float data1[8] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};
-    float data2[8] = {8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f};
-
-    // 데이터를 SIMD 벡터로 로드
-    __m256 vector1 = _mm256_loadu_ps(data1);
-    __m256 vector2 = _mm256_loadu_ps(data2);
-
-    // ========================================================================
-    // 1. 덧셈
-    // ========================================================================
-    std::cout << "1. 덧셈 (_mm256_add_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "두 벡터의 대응하는 요소들을 더합니다." << std::endl;
-    std::cout << std::endl;
-
-    print_m256(vector1, "벡터 1");
-    print_m256(vector2, "벡터 2");
-
-    // 덧셈 수행 (8개 요소를 동시에)
-    __m256 add_result = _mm256_add_ps(vector1, vector2);
-    print_m256(add_result, "덧셈 결과 (벡터 1 + 벡터 2)");
-
-    // 성능 비교: 스칼라 vs. SIMD
-    auto scalar_add = [&]() {
-        float result[8];
-        for (int i = 0; i < 8; i++) {
-            result[i] = data1[i] + data2[i];  // 8번 반복
-        }
-    };
-
-    auto simd_add = [&]() {
-        __m256 result = _mm256_add_ps(vector1, vector2);  // 1번에 8개 처리
-    };
-
-    benchmark_comparison("덧셈", scalar_add, simd_add);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 2. 뺄셈
-    // ========================================================================
-    std::cout << "2. 뺄셈 (_mm256_sub_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "두 벡터의 대응하는 요소들을 뺍니다." << std::endl;
-    std::cout << std::endl;
-
-    // 뺄셈 수행
-    __m256 sub_result = _mm256_sub_ps(vector1, vector2);
-    print_m256(sub_result, "뺄셈 결과 (벡터 1 - 벡터 2)");
-
-    // 성능 비교: 스칼라 vs. SIMD
-    auto scalar_sub = [&]() {
-        float result[8];
-        for (int i = 0; i < 8; i++) {
-            result[i] = data1[i] - data2[i];
-        }
-    };
-
-    auto simd_sub = [&]() {
-        __m256 result = _mm256_sub_ps(vector1, vector2);
-    };
-
-    benchmark_comparison("뺄셈", scalar_sub, simd_sub);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 3. 곱셈
-    // ========================================================================
-    std::cout << "3. 곱셈 (_mm256_mul_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "두 벡터의 대응하는 요소들을 곱합니다." << std::endl;
-    std::cout << std::endl;
-
-    // 곱셈 수행
-    __m256 mul_result = _mm256_mul_ps(vector1, vector2);
-    print_m256(mul_result, "곱셈 결과 (벡터 1 * 벡터 2)");
-
-    // 성능 비교: 스칼라 vs. SIMD
-    auto scalar_mul = [&]() {
-        float result[8];
-        for (int i = 0; i < 8; i++) {
-            result[i] = data1[i] * data2[i];
-        }
-    };
-
-    auto simd_mul = [&]() {
-        __m256 result = _mm256_mul_ps(vector1, vector2);
-    };
-
-    benchmark_comparison("곱셈", scalar_mul, simd_mul);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 4. 나눗셈
-    // ========================================================================
-    std::cout << "4. 나눗셈 (_mm256_div_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "두 벡터의 대응하는 요소들을 나눕니다." << std::endl;
-    std::cout << std::endl;
-
-    // 나눗셈 수행
-    __m256 div_result = _mm256_div_ps(vector1, vector2);
-    print_m256(div_result, "나눗셈 결과 (벡터 1 / 벡터 2)");
-
-    // 성능 비교: 스칼라 vs. SIMD
-    auto scalar_div = [&]() {
-        float result[8];
-        for (int i = 0; i < 8; i++) {
-            result[i] = data1[i] / data2[i];
-        }
-    };
-
-    auto simd_div = [&]() {
-        __m256 result = _mm256_div_ps(vector1, vector2);
-    };
-
-    benchmark_comparison("나눗셈", scalar_div, simd_div);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 5. 융합 곱셈-덧셈 (FMA)
-    // ========================================================================
-    std::cout << "5. 융합 곱셈-덧셈 (_mm256_fmadd_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "융합 곱셈-덧셈 연산을 수행: a*b + c" << std::endl;
-    std::cout << "별도의 곱셈과 덧셈보다 더 정확하고 빠릅니다." << std::endl;
-    std::cout << std::endl;
-
-    // FMA용 세 번째 벡터 생성
-    __m256 vector3 = _mm256_set1_ps(2.0f);
-    print_m256(vector3, "벡터 3");
-
-    // FMA 수행: vector1 * vector2 + vector3
-    // 1개 명령어로 곱셈과 덧셈을 동시에 수행 (더 빠르고 정확)
-    __m256 fma_result = _mm256_fmadd_ps(vector1, vector2, vector3);
-    print_m256(fma_result, "FMA 결과 (벡터 1 * 벡터 2 + 벡터 3)");
-
-    // 성능 비교: 스칼라 vs. SIMD
-    auto scalar_fma = [&]() {
-        float result[8];
-        for (int i = 0; i < 8; i++) {
-            result[i] = data1[i] * data2[i] + 2.0f;  // 2개 연산
-        }
-    };
-
-    auto simd_fma = [&]() {
-        __m256 result = _mm256_fmadd_ps(vector1, vector2, vector3);  // 1개 명령어
-    };
-
-    benchmark_comparison("융합 곱셈-덧셈", scalar_fma, simd_fma);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 6. 제곱근
-    // ========================================================================
-    std::cout << "6. 제곱근 (_mm256_sqrt_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "벡터의 각 요소에 대한 제곱근을 계산합니다." << std::endl;
-    std::cout << std::endl;
-
-    // 양수 값들로 벡터 생성
-    __m256 pos_vector = _mm256_set_ps(64.0f, 49.0f, 36.0f, 25.0f, 16.0f, 9.0f, 4.0f, 1.0f);
-    print_m256(pos_vector, "입력 벡터");
-
-    // 제곱근 계산 (8개 동시에)
-    __m256 sqrt_result = _mm256_sqrt_ps(pos_vector);
-    print_m256(sqrt_result, "제곱근 결과");
-
-    // 성능 비교: 스칼라 vs. SIMD
-    auto scalar_sqrt = [&]() {
-        float result[8];
-        union {
-            __m256 v;
-            float a[8];
-        } u;
-        u.v = pos_vector;
-        for (int i = 0; i < 8; i++) {
-            result[i] = std::sqrt(u.a[i]);  // 8번의 sqrt 호출
-        }
-    };
-
-    auto simd_sqrt = [&]() {
-        __m256 result = _mm256_sqrt_ps(pos_vector);  // 1번에 8개 처리
-    };
-
-    benchmark_comparison("제곱근", scalar_sqrt, simd_sqrt);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 7. 최소/최대 연산
-    // ========================================================================
-    std::cout << "7. 최소/최대 연산 (_mm256_min_ps, _mm256_max_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "대응하는 요소들의 최소값 또는 최대값을 계산합니다." << std::endl;
-    std::cout << std::endl;
-
-    print_m256(vector1, "벡터 1");
-    print_m256(vector2, "벡터 2");
-
-    // 최소값과 최대값 계산
-    __m256 min_result = _mm256_min_ps(vector1, vector2);
-    __m256 max_result = _mm256_max_ps(vector1, vector2);
-
-    print_m256(min_result, "최소값 결과");
-    print_m256(max_result, "최대값 결과");
-
-    // 성능 비교: 스칼라 vs. SIMD (최소값)
-    auto scalar_min = [&]() {
-        float result[8];
-        for (int i = 0; i < 8; i++) {
-            result[i] = std::min(data1[i], data2[i]);
-        }
-    };
-
-    auto simd_min = [&]() {
-        __m256 result = _mm256_min_ps(vector1, vector2);
-    };
-
-    benchmark_comparison("최소값", scalar_min, simd_min);
-    std::cout << std::endl;
-
-    // ========================================================================
-    // 8. 수평 연산 (Horizontal Operations)
-    // ========================================================================
-    std::cout << "8. 수평 연산 (_mm256_hadd_ps, _mm256_hsub_ps)" << std::endl;
-    std::cout << "---------------------------------------------------" << std::endl;
-    std::cout << "인접한 요소들의 수평 덧셈 또는 뺄셈을 수행합니다." << std::endl;
-    std::cout << std::endl;
-
-    // 테스트 벡터 생성
-    __m256 hadd_vec1 = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);
-    __m256 hadd_vec2 = _mm256_set_ps(16.0f, 15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f);
-
-    print_m256(hadd_vec1, "벡터 A");
-    print_m256(hadd_vec2, "벡터 B");
-
-    // 수평 덧셈 수행
-    // 인접한 쌍을 더함: (a0+a1, a2+a3, b0+b1, b2+b3, a4+a5, a6+a7, b4+b5, b6+b7)
-    __m256 hadd_result = _mm256_hadd_ps(hadd_vec1, hadd_vec2);
-    print_m256(hadd_result, "수평 덧셈 결과");
-
-    // 수평 뺄셈 수행
-    // 인접한 쌍을 뺌: (a0-a1, a2-a3, b0-b1, b2-b3, a4-a5, a6-a7, b4-b5, b6-b7)
-    __m256 hsub_result = _mm256_hsub_ps(hadd_vec1, hadd_vec2);
-    print_m256(hsub_result, "수평 뺄셈 결과");
-
-    // 참고: 수평 연산은 일반적으로 수직 연산보다 느립니다
-    // 내적(dot product)이나 행렬 연산 같은 특정 알고리즘에 유용합니다
-
-    return 0;
-}
diff --git a/cuda/helloworld/gpu_info.cu b/cuda/helloworld/gpu_info.cu
deleted file mode 100644
index e271a20..0000000
--- a/cuda/helloworld/gpu_info.cu
+++ /dev/null
@@ -1,129 +0,0 @@
-/*
- * gpu_info.cu
- *
- * CUDA GPU 정보 조회 프로그램
- * 시스템에 설치된 모든 NVIDIA GPU의 상세 정보를 출력
- *
- */
-
-#include <stdio.h>
-#include <cuda_runtime.h>
-
-int main() {
-    // GPU 개수 조회
-    int nDevices;
-    cudaGetDeviceCount(&nDevices);
-
-    printf("========================================\n");
-    printf("  CUDA 지원 GPU 개수: %d\n", nDevices);
-    printf("========================================\n\n");
-
-    // GPU가 없는 경우
-    if (nDevices == 0) {
-        printf("CUDA 지원 GPU를 찾을 수 없습니다.\n");
-        return 1;
-    }
-
-    // 각 GPU의 상세 정보 출력
-    for (int i = 0; i < nDevices; i++) {
-        cudaDeviceProp prop;
-        cudaGetDeviceProperties(&prop, i);  // GPU 속성 조회
-
-        printf("┌─────────────────────────────────────┐\n");
-        printf("│  GPU #%d 정보                        │\n", i);
-        printf("└─────────────────────────────────────┘\n\n");
-
-        // === 기본 정보 ===
-        printf("[ 기본 정보 ]\n");
-        printf("  Device Number: %d\n", i);
-        printf("  Device name: %s\n", prop.name);
-        printf("  Compute capability: %d.%d\n", prop.major, prop.minor);
-        // Compute capability: GPU 아키텍처 버전
-        // 7.5 = Turing, 8.0 = Ampere, 8.6 = Ada 등
-        printf("\n");
-
-        // === 메모리 정보 ===
-        printf("[ 메모리 정보 ]\n");
-        printf("  Total global memory: %lu bytes (%.2f GB)\n",
-               prop.totalGlobalMem,
-               prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));
-        // 전체 GPU 메모리 크기
-
-        printf("  Memory Clock Rate (KHz): %d\n", prop.memoryClockRate);
-        // 메모리 클럭 속도 (KHz 단위)
-
-        printf("  Memory Bus Width (bits): %d\n", prop.memoryBusWidth);
-        // 메모리 버스 폭 (비트 단위)
-        // 일반적으로 256bit, 384bit 등
-
-        // 피크 메모리 대역폭 계산
-        // 공식: 2 × Clock(GHz) × BusWidth(bytes) = GB/s
-        printf("  Peak Memory Bandwidth (GB/s): %.2f\n",
-               2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1.0e6);
-        // 2.0 = DDR (Double Data Rate)
-        // /8 = bits → bytes 변환
-        // /1.0e6 = KHz → GHz 변환
-        printf("\n");
-
-        // === 연산 유닛 정보 ===
-        printf("[ 연산 유닛 ]\n");
-        printf("  Number of SMs: %d\n", prop.multiProcessorCount);
-        // SM (Streaming Multiprocessor): GPU의 코어 개수
-        // 각 SM은 여러 CUDA 코어를 포함
-
-        printf("  Max threads per block: %d\n", prop.maxThreadsPerBlock);
-        // 하나의 블록이 가질 수 있는 최대 스레드 수
-        // 보통 1024개
-
-        printf("  Max threads dimensions:\n");
-        printf("    x = %d, y = %d, z = %d\n",
-               prop.maxThreadsDim[0],
-               prop.maxThreadsDim[1],
-               prop.maxThreadsDim[2]);
-        // 블록 내 스레드의 3차원 최대 크기
-        // 예: <<<grid, (1024, 1024, 64)>>> 같은 설정의 한계
-
-        printf("  Max grid dimensions:\n");
-        printf("    x = %d, y = %d, z = %d\n",
-               prop.maxGridSize[0],
-               prop.maxGridSize[1],
-               prop.maxGridSize[2]);
-        // 그리드의 3차원 최대 크기
-        // 예: <<<(2147483647, 65535, 65535), threads>>> 같은 설정의 한계
-        printf("\n");
-
-        // === 추가 유용한 정보 ===
-        printf("[ 추가 정보 ]\n");
-        printf("  Warp size: %d\n", prop.warpSize);
-        // Warp 크기 (보통 32개 스레드)
-
-        printf("  Shared memory per block: %zu bytes (%.2f KB)\n",
-               prop.sharedMemPerBlock,
-               prop.sharedMemPerBlock / 1024.0);
-        // 블록당 사용 가능한 공유 메모리
-
-        printf("  Registers per block: %d\n", prop.regsPerBlock);
-        // 블록당 사용 가능한 레지스터 개수
-
-        printf("  Max threads per SM: %d\n", prop.maxThreadsPerMultiProcessor);
-        // SM당 최대 스레드 수
-
-        printf("  Clock rate: %d KHz (%.2f GHz)\n",
-               prop.clockRate,
-               prop.clockRate / 1.0e6);
-        // GPU 코어 클럭 속도
-
-        printf("\n========================================\n\n");
-    }
-
-    printf("[ 요약 ]\n");
-    printf("시스템에서 사용 가능한 GPU: %d개\n", nDevices);
-
-    if (nDevices > 0) {
-        cudaDeviceProp prop;
-        cudaGetDeviceProperties(&prop, 0);
-        printf("기본 GPU: %s\n", prop.name);
-    }
-
-    return 0;
-}
diff --git a/cuda/helloworld/helloworld.cu b/cuda/helloworld/helloworld.cu
deleted file mode 100644
index 1688bdd..0000000
--- a/cuda/helloworld/helloworld.cu
+++ /dev/null
@@ -1,26 +0,0 @@
-#include "cuda_runtime.h"
-#include "device_launch_parameters.h"
-#include <stdio.h>
-
-__global__ void test01()  // GPU에서 실행되는 커널
-{
-    int warp_ID_Value = threadIdx.x / 32;
-    //           스레드 ID ÷ 32 = Warp ID
-    //           0~31 → Warp 0
-    //           32~63 → Warp 1
-
-    printf("block=%d thread=%d warp=%d\n",
-           blockIdx.x,   // 블록 번호 (0 또는 1)
-           threadIdx.x,  // 블록 내 스레드 번호 (0~63)
-           warp_ID_Value);
-}
-
-int main()
-{
-    test01<<<2, 64>>>();  // 2개 블록, 블록당 64개 스레드
-    //     │  └─ 블록당 스레드 수
-    //     └──── 블록 개수
-
-    cudaDeviceSynchronize();  // GPU 작업 완료 대기
-    return 0;
-}
diff --git a/cuda/helloworld/matrix_mul.cu b/cuda/helloworld/matrix_mul.cu
deleted file mode 100644
index 503dbbb..0000000
--- a/cuda/helloworld/matrix_mul.cu
+++ /dev/null
@@ -1,214 +0,0 @@
-#include <stdio.h>
-#include <stdlib.h>
-#include <sys/time.h>
-
-// 타일 크기 (32 * 32)
-// Warp 크기(32)의 배수로 설정하여 효율 극대화
-#define TILE_WIDTH 32
-
-/*
- * CUDA 커널: Tiled 행렬 곱셈
- *
- * 최적화 전략:
- * 1. Global Memory → Shared Memory로 타일 단위 복사
- * 2. Shared Memory에서 반복 계산 (빠른 접근)
- * 3. 타일 단위로 순회하며 부분 결과 누적
- *
- * 파라미터:
- *   a, b: 입력 행렬 (GPU 메모리)
- *   c: 출력 행렬 (GPU 메모리)
- *   width: 행렬 크기 (정사각 행렬)
- */
-__global__ void matrixMul(int *a, int *b, int *c, int width) {
-    // Shared Memory 선언
-    // 블록 내 모든 스레드가 공유하는 고속 메모리
-    // 32 X 32 타일 2개 (A용, B용)
-    __shared__ int ds_A[TILE_WIDTH][TILE_WIDTH];
-    __shared__ int ds_B[TILE_WIDTH][TILE_WIDTH];
-
-    // thread index 계산
-    int bx = blockIdx.x; // block X 좌표
-    int by = blockIdx.y; // block Y 좌표
-    int tx = threadIdx.x; // block 내 thread X 좌표
-    int ty = threadIdx.y; // block 내 thread Y 좌표
-
-    // 전역 행렬에서 이 thread 가 계산할 위치
-    int Row = by * TILE_WIDTH + ty; // 결과 행렬 C의 행 번호
-    int Col = bx * TILE_WIDTH + tx; // 결과 행렬 C의 열 번호
-
-    // 이 thread가 계산할 C[Row][Col]의 누적값
-    int Cvalue = 0;
-
-    // 타일 단위 순회
-    // 행렬을 TILE_WIDTH * TILE_WIDTH 크기의 타일로 나눔
-    // 전체 타일 개수 = width / TILE_WIDTH
-    for (int t = 0; t < width / TILE_WIDTH; ++t) {
-        // 1. Global → Shared Memory 복사
-        // A 행렬의 t번째 타일을 Shared Memory로 복사
-        // A[Row][t*TILE_WIDTH + tx]
-        ds_A[ty][tx] = a[Row * width + t * TILE_WIDTH + tx];
-
-        // B 행렬의 t번째 타일을 Shared Memory로 복사
-        // B[t*TILE_WIDTH + ty][Col]
-        ds_B[ty][tx] = b[(t * TILE_WIDTH +ty) * width + Col];
-
-        // 동기화
-        // 블록 내 모든 스레드가 복사 완료할 때까지 대기
-        // 모든 thread가 ds_A, ds_B 를 안전하게 읽을 수 있도록 보장
-        __syncthreads();
-
-        // 2. Shared Memory 에서 계산
-        // 타일 내에서 내적 계산
-        // Cvalue += A의 행 * B의 열
-        for (int k = 0;k < TILE_WIDTH; ++k) {
-            Cvalue += ds_A[ty][k] * ds_B[k][tx];
-            // ds_A[ty][k]: A 행렬의 현재 행에서 k번째 요소
-            // ds_B[k][tx]: B 행렬의 k번째 행, 현재 열
-        }
-
-        // 동기화
-        // 다음 타일로 넘어가기전에 모든 계산 완료대기
-        // ds_A, ds_B를 덮어쓰기 전에 모든 thread 가 사용 완료하도록 보장
-        __syncthreads();
-
-    }
-
-    // 결과를 global memory에 저장
-    // 모든 타일 순회 후 최종값 저장
-    c[Row * width + Col] = Cvalue;
-}
-
-void matrixMulCPU(int *a, int *b, int *c, int width) {
-    for (int i = 0; i < width; i++) {
-        for (int j = 0; j < width; j++) {
-            int sum = 0;
-            for (int k = 0; k < width; k++) {
-                sum += a[i * width + k] * b[k * width + j];
-            }
-            c[i * width + j] = sum;
-        }
-    }
-}
-
-/*
- * 행렬 초기화
- *
- * A[i][j] = i * width + j
- * B[i][j] = j * width + i (A의 전치 패턴)
- * C[i][j] = 0 (초기화)
- */
-void matrixInit(int *a, int *b, int *c, int width) {
-    for (int i = 0; i < width; ++ i) {
-        for (int j = 0; j < width; ++j) {
-            a[i * width + j] = i * width + j;
-            b[i * width + j] = j * width + i;
-            c[i * width + j] = 0;
-        }
-    }
-}
-
-long long getCurrentTime() {
-    struct timeval tv;
-    gettimeofday(&tv, NULL);
-    return (long long)tv.tv_sec * 1000000LL + (long long)tv.tv_usec;
-}
-
-int main() {
-    int *a, *b, *c; // 호스트(CPU) 메모리
-    int *d_a, *d_b, *d_c; // 디바이스(GPU) 메모리
-
-    // 행렬 크기: 16384 × 16384 (4096 * 4)
-    int width = 4096 * 4;
-    int size = width * width * sizeof(int);
-
-    printf("========================================\n");
-    printf("  Tiled 행렬 곱셈 (Shared Memory 사용)\n");
-    printf("========================================\n");
-    printf("행렬 크기: %d × %d\n", width, width);
-    printf("메모리 사용량: %.2f GB (행렬당)\n", size / (1024.0 * 1024.0 * 1024.0));
-    printf("타일 크기: %d × %d\n", TILE_WIDTH, TILE_WIDTH);
-    printf("\n");
-
-    // host memory 할당
-    a = (int *)malloc(size);
-    b = (int *)malloc(size);
-    c = (int *)malloc(size);
-
-    if (!a || !b || !c) {
-        printf("CPU 메모리 할당 실패\n");
-        return 1;
-    }
-
-    // GPU Memory 할당
-    cudaMalloc((void **)&d_a, size);
-    cudaMalloc((void **)&d_b, size);
-    cudaMalloc((void **)&d_c, size);
-
-    // 행렬 초기화
-    matrixInit(a, b, c, width);
-
-    // 데이터 복사 CPU -> GPU
-    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
-    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);
-
-    // kernel 설정
-    // block: 32 * 32 thread (1024 개 thread)
-    dim3 dimBlock(TILE_WIDTH, TILE_WIDTH);
-
-    // grid: 필요한 block 개수 계산(올림 나눗셈)
-    // 16384 ÷ 32 = 512개 블록 (가로, 세로 각각)
-    dim3 dimGrid((width + TILE_WIDTH - 1) / TILE_WIDTH,
-            (width + TILE_WIDTH - 1) / TILE_WIDTH);
-
-    printf("\n커널 설정:\n");
-    printf("  블록: %d × %d 스레드\n", TILE_WIDTH, TILE_WIDTH);
-    printf("  그리드: %d × %d 블록\n", dimGrid.x, dimGrid.y);
-    printf("  총 스레드: %d개\n", dimGrid.x * dimGrid.y * TILE_WIDTH * TILE_WIDTH);
-
-    // kernel 실행 (시간 측정)
-    printf("\n행렬 곱셈 실행 중...\n");
-
-    long long start_time = getCurrentTime();
-
-    // kernel 실행
-    matrixMul<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, width);
-
-    // GPU 작업 완료 대기
-    cudaDeviceSynchronize();
-
-    long long end_time = getCurrentTime();
-
-    // 실행 시간 계산
-    double execution_time = (double)(end_time - start_time) / 1000000.0;
-
-    printf("실행 시간: %.6f 초\n", execution_time);
-
-    // FLOPS 계산 (Floating Point Operations Per Second)
-    // 행렬 곱셈: width^3 * 2 (곱셈 + 덧셈)
-    long long operations = 2LL * width * width * width;
-    double gflops = (operations / execution_time) / 1e9;
-    printf("성능: %.2f GFLOPS\n", gflops);
-
-    // 결과 복사: GPU → CPU
-    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);
-
-    // 결과 검증 (일부)
-    printf("\n결과 검증 (C[0][0] ~ C[2][2]):\n");
-    for (int i = 0; i < 3; i++) {
-        for (int j = 0; j < 3; j++) {
-            printf("%10d ", c[i * width + j]);
-        }
-        printf("\n");
-    }
-
-    // === 메모리 해제 ===
-    cudaFree(d_a);
-    cudaFree(d_b);
-    cudaFree(d_c);
-    free(a);
-    free(b);
-    free(c);
-
-    return 0;
-}
-
diff --git a/cuda/helloworld/vector_add.cu b/cuda/helloworld/vector_add.cu
deleted file mode 100644
index 7cc76f3..0000000
--- a/cuda/helloworld/vector_add.cu
+++ /dev/null
@@ -1,153 +0,0 @@
-/*
- * vector_add.cu
- *
- * CUDA를 사용한 벡터 덧셈 예제
- * 두 개의 큰 벡터를 GPU에서 병렬로 더하는 프로그램
- *
- */
-
-#include <stdio.h>
-#include <cuda_runtime.h>
-
-// 벡터 크기 정의: 32MB (1024 * 1024 * 32 개의 int)
-#define SIZE 1024*1024*32
-
-/*
- * CUDA 커널: 벡터 덧셈
- *
- * 각 스레드가 벡터의 한 요소를 담당
- * C[i] = A[i] + B[i]
- *
- * 파라미터:
- *   A, B: 입력 벡터 (GPU 메모리)
- *   C: 출력 벡터 (GPU 메모리)
- *   n: 벡터의 크기
- */
-__global__ void vectorAdd(int *A, int *B, int *C, int n) {
-    // 전역 스레드 ID 계산
-    // threadIdx.x: 블록 내 스레드 번호
-    // blockDim.x: 블록당 스레드 개수
-    // blockIdx.x: 블록 번호
-    int i = threadIdx.x + blockDim.x * blockIdx.x;
-
-    // 배열 범위를 벗어나지 않도록 체크
-    if (i < n) {
-        C[i] = A[i] + B[i];
-    }
-}
-
-int main() {
-    // === 1. 변수 선언 ===
-    int *A, *B, *C;            // 호스트(CPU) 메모리 포인터
-    int *d_A, *d_B, *d_C;      // 디바이스(GPU) 메모리 포인터
-    int size = SIZE * sizeof(int);  // 바이트 단위 크기
-
-    // CUDA 이벤트 생성 (실행 시간 측정용)
-    cudaEvent_t start, stop;
-    cudaEventCreate(&start);
-    cudaEventCreate(&stop);
-
-
-    // === 2. 호스트 메모리 할당 및 초기화 ===
-    printf("벡터 크기: %d 개 (%.2f MB)\n", SIZE, size / (1024.0 * 1024.0));
-
-    A = (int *)malloc(size);
-    B = (int *)malloc(size);
-    C = (int *)malloc(size);
-
-    // 벡터 초기화
-    // A[i] = i, B[i] = SIZE - i
-    for (int i = 0; i < SIZE; i++) {
-        A[i] = i;
-        B[i] = SIZE - i;
-    }
-
-
-    // === 3. GPU 메모리 할당 ===
-    printf("GPU 메모리 할당 중...\n");
-    cudaMalloc((void **)&d_A, size);
-    cudaMalloc((void **)&d_B, size);
-    cudaMalloc((void **)&d_C, size);
-
-
-    // === 4. 데이터를 CPU에서 GPU로 복사 ===
-    printf("데이터 복사: CPU -> GPU\n");
-    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
-    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);
-
-
-    // === 5. 커널 실행 설정 ===
-    int threadsPerBlock = 96;  // 블록당 96개 스레드
-    // 블록 개수 계산: 올림 나눗셈 (모든 요소를 커버하도록)
-    int blocksPerGrid = (SIZE + threadsPerBlock - 1) / threadsPerBlock;
-
-    printf("\n커널 설정:\n");
-    printf("  블록당 스레드: %d\n", threadsPerBlock);
-    printf("  총 블록 개수: %d\n", blocksPerGrid);
-    printf("  총 스레드: %d\n", blocksPerGrid * threadsPerBlock);
-
-
-    // === 6. 커널 실행 (시간 측정) ===
-    cudaEventRecord(start);  // 타이머 시작
-
-    // 커널 실행
-    // <<<블록 개수, 블록당 스레드 개수>>>
-    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, SIZE);
-
-    cudaEventRecord(stop);   // 타이머 종료
-
-
-    // === 7. 결과를 GPU에서 CPU로 복사 ===
-    printf("\n데이터 복사: GPU -> CPU\n");
-    cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);
-
-
-    // === 8. 실행 시간 계산 및 출력 ===
-    cudaEventSynchronize(stop);  // GPU 작업 완료 대기
-
-    float milliseconds = 0;
-    cudaEventElapsedTime(&milliseconds, start, stop);
-
-    printf("\n=== 결과 ===\n");
-    printf("실행 시간: %.3f ms\n", milliseconds);
-    printf("처리량: %.2f GB/s\n",
-           (3.0 * size) / (milliseconds / 1000.0) / (1024.0 * 1024.0 * 1024.0));
-    // 3.0 = A 읽기 + B 읽기 + C 쓰기
-
-
-    // === 9. 결과 검증 (처음 10개 출력) ===
-    printf("\n처음 10개 요소:\n");
-    printf("%-10s %-10s %-10s %-10s\n", "Index", "A", "B", "C (A+B)");
-    printf("--------------------------------------------\n");
-    for(int i = 0; i < 10; i++) {
-        printf("%-10d %-10d %-10d %-10d", i, A[i], B[i], C[i]);
-
-        // 검증: C[i]가 A[i] + B[i]와 같은지 확인
-        if (C[i] == A[i] + B[i]) {
-            printf(" ✓\n");
-        } else {
-            printf(" ✗ 오류!\n");
-        }
-    }
-
-
-    // === 10. 메모리 해제 ===
-
-    // GPU 메모리 해제
-    cudaFree(d_A);
-    cudaFree(d_B);
-    cudaFree(d_C);
-
-    // CPU 메모리 해제
-    free(A);
-    free(B);
-    free(C);
-
-    // CUDA 이벤트 삭제
-    cudaEventDestroy(start);
-    cudaEventDestroy(stop);
-
-    printf("완료!\n");
-
-    return 0;
-}
diff --git a/cuda/profiling/README.md b/cuda/profiling/README.md
deleted file mode 100644
index ec9c240..0000000
--- a/cuda/profiling/README.md
+++ /dev/null
@@ -1,141 +0,0 @@
-
-## Memory 구조
-
-- CPU와 GPU는 완전히 분리된 메모리 공간을 사용한다.
-    ```
-    CPU 메모리          GPU 메모리
-    ┌─────────┐        ┌─────────┐
-    │  A[0]   │        │         │
-    │  A[1]   │  복사→ │  d_A[0] │
-    │  A[2]   │  ====> │  d_A[1] │
-    │   ...   │        │  d_A[2] │
-    └─────────┘        │   ...   │
-                       └─────────┘
-    ```
-
-```
-CPU (호스트)
-  RAM ← 여기에 int *A, *B, *C
-
-     ↕ cudaMemcpy로 복사
-
-GPU (디바이스)
-  ┌─────────────────────────┐
-  │ Global Memory (전역)    │ ← cudaMalloc으로 할당 (d_A, d_B, d_C)
-  │ - 크기: 수 GB           │    모든 스레드가 접근 가능
-  │ - 속도: 느림            │    하지만 가장 느림
-  ├─────────────────────────┤
-  │ Shared Memory (공유)    │ ← __shared__ 선언
-  │ - 크기: 블록당 48KB     │    블록 내 스레드만 공유
-  │ - 속도: 100배 빠름      │
-  ├─────────────────────────┤
-  │ Registers (레지스터)    │ ← int i = ... 같은 지역변수
-  │ - 크기: 매우 작음       │    각 스레드 전용
-  │ - 속도: 가장 빠름       │
-  └─────────────────────────┘
-```
-
-
-### cudaMemcpy(dst, src, size, direction);
-
-- cudaMemcpyHostToDevice   // CPU → GPU (입력 데이터)
-- cudaMemcpyDeviceToHost   // GPU → CPU (결과 받기)
-- cudaMemcpyDeviceToDevice // GPU → GPU (같은 GPU 내)
-- cudaMemcpyHostToHost     // CPU → CPU (거의 안 씀)
-
-### GPU Memory Hierachy
-
-```
-GPU 칩 내부:
-┌────────────────────────────────────────┐
-│                                        │
-│  ┌──────────────────────────────┐      │
-│  │    Global Memory (VRAM)      │      │ ← GPU 보드의 GDDR6 메모리
-│  │    크기: 8GB ~ 80GB          │      │   (GPU 외부이지만 전용)
-│  └──────────────────────────────┘      │
-│                                        │
-│  SM 0          SM 1          SM 2      │
-│  ┌────┐       ┌────┐       ┌────┐      │
-│  │Reg │       │Reg │       │Reg │      │ ← 레지스터
-│  │Reg │       │Reg │       │Reg │      │   (SM 내부, 빠름)
-│  ├────┤       ├────┤       ├────┤      │
-│  │Shar│       │Shar│       │Shar│      │ ← Shared Memory
-│  │ed  │       │ed  │       │ed  │      │   (SM 내부, 빠름)
-│  └────┘       └────┘       └────┘      │
-│                                        │
-│  ... (수십~수백 개 SM)                 │
-└────────────────────────────────────────┘
-```
-
-Global Memory (전역 메모리)
-- 위치: GPU 보드의 GDDR6/HBM 메모리 칩
-- 크기:
-    - RTX 3090: 24GB
-    - RTX 4090: 24GB
-    - A100: 40GB/80GB
-- 특징
-    ```cu
-    cudaMalloc(&d_A, size);  // 여기 할당됨
-
-    __global__ void kernel(int *d_A) {
-        d_A[i] = 10;  // 모든 스레드가 접근 가능
-    }
-    ```
-    - 모든 블록, 모든 스레드가 접근 가능
-    - 가장 느림 (400~900 GB/s)
-    - 가장 큼 (수십 GB)
-    - 물리적 위치: GPU 카드의 메모리 칩 (CPU RAM과는 별개)
-
-Shared Memory (공유 메모리)
-- 위치: 각 SM(Streaming Multiprocessor) 내부
-- 크기: 블록당 48KB~164KB
-- 특징
-    ```cu
-    __global__ void kernel() {
-        __shared__ int ds_A[32][32];  // 여기 할당됨
-
-        // 같은 블록 내 스레드만 공유
-        ds_A[ty][tx] = 10;
-        __syncthreads();  // 동기화 필요
-    }
-    ```
-    - 블록 내 스레드들만 공유
-    - Global Memory보다 100배 빠름 (수십 TB/s)
-    - 크기 제한 있음 (블록당 수십 KB)
-    - 물리적 위치: SM(코어) 내부의 SRAM
-
-Registers (레지스터)
-- 위치: 각 SM 내부의 레지스터 파일
-- 크기: 스레드당 255개 (SM당 65,536개)
-- 특징
-    ```cu
-    __global__ void kernel() {
-        int i = threadIdx.x;  // 레지스터에 저장
-        int sum = 0;          // 레지스터에 저장
-        float x = 1.5f;       // 레지스터에 저장
-    }
-    ```
-    - 각 스레드 전용 (다른 스레드 접근 불가)
-    - 가장 빠름 (1 사이클)
-    - 가장 작음 (스레드당 ~1KB)
-    - 물리적 위치: SM 내부의 레지스터 파일
-
-실제 하드웨어 구조
-```
-RTX 3090 예시:
-┌────────────────────────────────────┐
-│ GPU 칩                             │
-│                                    │
-│ 82개 SM × 각 SM마다:               │
-│   - 레지스터: 65,536개             │
-│   - Shared Memory: 100KB           │
-│                                    │
-└────────────────────────────────────┘
-         ↕ PCIe 버스
-┌────────────────────────────────────┐
-│ GDDR6X 메모리 (24GB)               │ ← Global Memory
-│ 대역폭: 936 GB/s                   │
-└────────────────────────────────────┘
-```
-
-
diff --git a/cuda/profiling/vector_add.cu b/cuda/profiling/vector_add.cu
deleted file mode 100644
index c19136d..0000000
--- a/cuda/profiling/vector_add.cu
+++ /dev/null
@@ -1,138 +0,0 @@
-/*
- * CUDA 벡터 덧셈 프로그램
- * 두 개의 큰 벡터(각 32M 요소)를 GPU에서 병렬로 더함
- */
-
-#include <stdio.h>
-#include <cuda_runtime.h>
-
-#define SIZE 1024*1024*32
-
-/*
- * CUDA 커널: 벡터 덧셈
- *
- * 각 스레드가 벡터의 한 요소를 담당
- * C[i] = A[i] + B[i]
- *
- * 파라미터:
- *   A, B: 입력 벡터 (GPU 메모리)
- *   C: 출력 벡터 (GPU 메모리)
- *   n: 벡터 크기
- */
-__global__ void vectorAdd(int *A, int *B, int *C, int n) {
-    // 전역 thread ID 계산
-    // 이 thread 가 처리할 배열 인덱스
-    int i = threadIdx.x + blockDim.x * blockIdx.x;
-
-    // 배열 범위를 벗어나지 않도록 체크
-    // 마지막 블록은 일부 스레드만 사용
-    if (i < n) {
-        C[i] = A[i] + B[i];
-    }
-}
-
-int main() {
-    int *A, *B, *C;
-    int *d_A, *d_B, *d_C;
-    int size = SIZE * sizeof(int);
-
-    // CUDA 이벤트 생성
-    // GPU timer (cudaEventElapsedTime으로 정확한 커널 실행 시간 측정)
-    cudaEvent_t start, stop;
-    cudaEventCreate(&start);
-    cudaEventCreate(&stop);
-
-    // host memory 할당
-    A = (int *)malloc(size);
-    B = (int *)malloc(size);
-    C = (int *)malloc(size);
-
-    // vector 초기화
-    // A[i] = i (0, 1, 2, 3, ...)
-    // B[i] = SIZE - i (33554432, 33554431, 33554430, ...)
-    for (int i = 0; i < SIZE; i++) {
-        A[i] = i;
-        B[i ] = SIZE - i;
-    }
-    // 예상 결과: C[i] = i + (SIZE - i) = SIZE (모든 요소가 33554432)
-
-    // GPU memory 할당
-    // cudaMalloc: GPU 전역 메모리에 공간 할당
-    cudaMalloc((void **)&d_A, size);
-    cudaMalloc((void **)&d_B, size);
-    cudaMalloc((void **)&d_C, size);
-
-    // data 복사 cpu -> gpu
-    // cudaMemcpyHostToDevice: host -> device
-    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
-    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);
-
-    // timer start
-    cudaEventRecord(start);
-
-    // kernel 실행 설정
-    int threadsPerBlock = 96; // 블록당 96개 스레드
-
-    // 블록 개수 계산 (올림 나눗셈)
-    // 모든 요소를 처리하기 위해 필요한 블록 수
-    // (33554432 + 96 - 1) / 96 = 349526 블록
-    int blocksPerGrid = (SIZE + threadsPerBlock - 1) / threadsPerBlock;
-
-    // 커널 실행
-    // <<<블록 개수, 블록당 스레드 개수>>>
-    // 총 스레드: 349526 * 96 = 33,554,496 (SIZE보다 약간 많음)
-    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, SIZE);
-
-    // 타이머 종료
-    // 커널 실행 완료
-    cudaEventRecord(stop);
-
-    // 결과 복사: GPU → CPU
-    // 계산 결과를 CPU 메모리로 가져옴
-    cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);
-
-    // 실행 시간 계산
-    // GPU 작업이 완전히 끝날 때까지 대기
-    cudaEventSynchronize(stop);
-
-    // start와 stop 사이의 시간 계산 (밀리초)
-    float milliseconds = 0;
-    cudaEventElapsedTime(&milliseconds, start, stop);
-
-    printf("=== CUDA 벡터 덧셈 ===\n");
-    printf("벡터 크기: %d 개 요소\n", SIZE);
-    printf("메모리 사용: %.2f MB (벡터당)\n", size / (1024.0 * 1024.0));
-    printf("블록: %d개\n", blocksPerGrid);
-    printf("블록당 스레드: %d개\n", threadsPerBlock);
-    printf("총 스레드: %d개\n", blocksPerGrid * threadsPerBlock);
-    printf("\n실행 시간: %f 밀리초\n", milliseconds);
-
-    // 결과 검증 (처음 10개 출력)
-    printf("\n처음 10개 요소:\n");
-    for(int i = 0; i < 10; i++) {
-        printf("A=%-8d\tB=%-8d\t--->\tC=%-8d", A[i], B[i], C[i]);
-
-        // 검증: C[i] = SIZE 여야 함
-        if (C[i] == SIZE) {
-            printf(" ✓\n");
-        } else {
-            printf(" ✗ (예상: %d)\n", SIZE);
-        }
-    }
-
-    // GPU 메모리 해제
-    cudaFree(d_A);
-    cudaFree(d_B);
-    cudaFree(d_C);
-
-    // CPU 메모리 해제
-    free(A);
-    free(B);
-    free(C);
-
-    // CUDA 이벤트 삭제
-    cudaEventDestroy(start);
-    cudaEventDestroy(stop);
-
-    return 0;
-}
diff --git a/include/simd_utils.h b/include/simd_utils.h
deleted file mode 100644
index 7eafdae..0000000
--- a/include/simd_utils.h
+++ /dev/null
@@ -1,154 +0,0 @@
-/**
- * simd_utils.h - Utility functions and macros for SIMD programming
- *
- * This header provides common utilities for SIMD programming, including:
- * - Type definitions for SIMD vectors
- * - Helper macros for alignment
- * - Utility functions for printing SIMD vectors
- * - Performance measurement utilities
- */
-
-#ifndef SIMD_UTILS_H
-#define SIMD_UTILS_H
-
-#include <immintrin.h> // AVX2, 256-bit operations
-#include <iostream>
-#include <iomanip>
-#include <chrono>
-#include <vector>
-#include <string>
-#include <functional>
-
-// Alignment macros
-#define SIMD_ALIGN_32 alignas(32)
-#define SIMD_ALIGN_64 alignas(64)
-
-// Helper union for accessing SIMD vector elements
-union float8 {
-    __m256 v;
-    float a[8];
-
-    float8(__m256 _v) : v(_v) {}
-    float8() : v(_mm256_setzero_ps()) {}
-};
-
-union double4 {
-    __m256d v;
-    double a[4];
-
-    double4(__m256d _v) : v(_v) {}
-    double4() : v(_mm256_setzero_pd()) {}
-};
-
-union int8 {
-    __m256i v;
-    int a[8];
-
-    int8(__m256i _v) : v(_v) {}
-    int8() : v(_mm256_setzero_si256()) {}
-};
-
-// Print utilities
-inline void print_m256(const __m256& v, const std::string& label = "") {
-    float8 tmp(v);
-    if (!label.empty()) {
-        std::cout << label << ": ";
-    }
-    std::cout << "[";
-    for (int i = 0; i < 7; i++) {
-        std::cout << tmp.a[i] << ", ";
-    }
-    std::cout << tmp.a[7] << "]" << std::endl;
-}
-
-inline void print_m256d(const __m256d& v, const std::string& label = "") {
-    double4 tmp(v);
-    if (!label.empty()) {
-        std::cout << label << ": ";
-    }
-    std::cout << "[";
-    for (int i = 0; i < 3; i++) {
-        std::cout << tmp.a[i] << ", ";
-    }
-    std::cout << tmp.a[3] << "]" << std::endl;
-}
-
-inline void print_m256i(const __m256i& v, const std::string& label = "") {
-    int8 tmp(v);
-    if (!label.empty()) {
-        std::cout << label << ": ";
-    }
-    std::cout << "[";
-    for (int i = 0; i < 7; i++) {
-        std::cout << tmp.a[i] << ", ";
-    }
-    std::cout << tmp.a[7] << "]" << std::endl;
-}
-
-// Performance measurement utilities
-class Timer {
-private:
-    std::chrono::high_resolution_clock::time_point start_time;
-    std::string label;
-
-public:
-    Timer(const std::string& _label = "Operation") : label(_label) {
-        start_time = std::chrono::high_resolution_clock::now();
-    }
-
-    ~Timer() {
-        auto end_time = std::chrono::high_resolution_clock::now();
-        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);
-        std::cout << label << " took " << duration.count() << " microseconds" << std::endl;
-    }
-};
-
-// Benchmark function to compare scalar vs SIMD implementations
-template<typename ScalarFunc, typename SimdFunc>
-void benchmark_comparison(
-    const std::string& label,
-    ScalarFunc scalar_func,
-    SimdFunc simd_func,
-    int iterations = 1000000
-) {
-    // Warm-up
-    scalar_func();
-    simd_func();
-
-    // Benchmark scalar implementation
-    auto scalar_start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < iterations; i++) {
-        scalar_func();
-    }
-    auto scalar_end = std::chrono::high_resolution_clock::now();
-    auto scalar_duration = std::chrono::duration_cast<std::chrono::microseconds>(scalar_end - scalar_start);
-
-    // Benchmark SIMD implementation
-    auto simd_start = std::chrono::high_resolution_clock::now();
-    for (int i = 0; i < iterations; i++) {
-        simd_func();
-    }
-    auto simd_end = std::chrono::high_resolution_clock::now();
-    auto simd_duration = std::chrono::duration_cast<std::chrono::microseconds>(simd_end - simd_start);
-
-    // Print results
-    std::cout << "===== " << label << " Benchmark =====" << std::endl;
-    std::cout << "Scalar implementation: " << scalar_duration.count() << " microseconds" << std::endl;
-    std::cout << "SIMD implementation: " << simd_duration.count() << " microseconds" << std::endl;
-
-    double speedup = static_cast<double>(scalar_duration.count()) / simd_duration.count();
-    std::cout << "Speedup: " << std::fixed << std::setprecision(2) << speedup << "x" << std::endl;
-    std::cout << "===============================" << std::endl;
-}
-
-// Allocate aligned memory
-template<typename T>
-T* aligned_alloc(size_t size, size_t alignment = 32) {
-    void* ptr = nullptr;
-    if (posix_memalign(&ptr, alignment, size * sizeof(T)) != 0) {
-        throw std::bad_alloc();
-    }
-    return static_cast<T*>(ptr);
-}
-
-#endif // SIMD_UTILS_H
